{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd089a54e7346d33656f2940aba0e47a561becf96fe36c69c035d9bc66d085d8900",
   "display_name": "Python 3.7.9 64-bit ('Master_Thesis_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h1 style='margin:10px 5px'> \n",
    "Master Thesis Yannik Haller - Bigram and Trigram Models\n",
    "</h1>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "1. Load required packages and the data\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Hallk\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\pylab\\config.py:70: DeprecationWarning: InlineBackend._figure_formats_changed is deprecated in traitlets 4.1: use @observe and @unobserve instead.\n  def _figure_formats_changed(self, name, old, new):\n"
     ]
    }
   ],
   "source": [
    "# Import required baseline packages\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Change pandas' setting to print out long strings\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Spacy (for lemmatization)\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim (optional)\n",
    "import logging\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the appropriate working directory\n",
    "os.chdir('D:\\\\Dropbox\\\\MA_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read in the fully preprocessed data\n",
    "def read_preprocessed(language, tokenize = True):\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "    \n",
    "    # Set the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "\n",
    "    # Define the name of the file to load\n",
    "    filename = \"Preprocessed/\"+language+\"_preprocessed.csv\"\n",
    "\n",
    "    # Read in the dataframe containing the text data\n",
    "    tx_pp = pd.read_csv(filename, index_col = 0, dtype = {'tx': object})\n",
    "\n",
    "    # Get the articles' index together with an enumeration to identify their position in the list of precleaned articles\n",
    "    idx = tx_pp.index\n",
    "    idx = pd.DataFrame(idx, columns = [language+'_idx'])\n",
    "\n",
    "    # Reduce the dataframe to a list containing the text data\n",
    "    tx_pp = tx_pp.tx.to_list()\n",
    "\n",
    "    # Tokenize the data again if tokenize = True (RAM-saving)\n",
    "    if tokenize:\n",
    "        tx_pp = retokenize(tx_pp)\n",
    "\n",
    "    # Return the preprocessed data\n",
    "    return tx_pp, idx\n",
    "\n",
    "# Define a function to retokenize the preprocessed text data (RAM-saving)\n",
    "def retokenize(article_list):\n",
    "    for i in range(len(article_list)):\n",
    "        temp_tx = str(article_list[i]).split()\n",
    "        article_list[i] = temp_tx\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15474568"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Read in the preprocessed data\n",
    "de_tx, de_idx = read_preprocessed('de')\n",
    "\n",
    "# Take a look at the size of the precleaned data\n",
    "sys.getsizeof(de_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['rückkehrer', 'stefan', 'meier', 'überragen', 'flames', 'herisau']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Take a look at the preprocessed data\n",
    "de_tx[0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          de_idx\n",
       "1934310  2441180\n",
       "1934311  2441181\n",
       "1934312  2441182"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>de_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1934310</th>\n      <td>2441180</td>\n    </tr>\n    <tr>\n      <th>1934311</th>\n      <td>2441181</td>\n    </tr>\n    <tr>\n      <th>1934312</th>\n      <td>2441182</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Take a look at the dataframe containing the according index (i.e. article id)\n",
    "de_idx.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1934310, 1934311]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Retrieve the location of the article in the preprocessed data using the according article id\n",
    "article_ids = [2441180, 2441181]\n",
    "location = de_idx[de_idx.de_idx.isin(article_ids)].index.tolist() #1934310\n",
    "\n",
    "# Access the preprocessed text from the articles with the article ids in [2441180, 2441181]\n",
    "#list(de_tx[i] for i in location)\n",
    "\n",
    "# Look at the according location of the articles with the article ids in [2441180, 2441181]\n",
    "location"
   ]
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2. Build bigram model\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing time to build bigrams : 5.357060194015503 minutes\n"
     ]
    }
   ],
   "source": [
    "# Build bigrams while keeping track of the processing time\n",
    "t = time.time()\n",
    "# Set up a loop to go through all articles\n",
    "for n in range(len(de_tx)):\n",
    "    # Create a temporary storage to store the bigrams of the currently processed article\n",
    "    temp_tx = []\n",
    "    # Set up a loop to go through all tokens of the article and create bigrams\n",
    "    for i in range(len(de_tx[n]) - 1):\n",
    "        temp_tx.append(de_tx[n][i] + \"_\" + de_tx[n][i+1])\n",
    "    # Overwrite the list of unigrams of the focal article with the corresponding list of bigrams\n",
    "    de_tx[n] = temp_tx\n",
    "print(\"Processing time to build bigrams :\", str((time.time() - t)/60), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['rückkehrer_stefan',\n",
       " 'stefan_meier',\n",
       " 'meier_überragen',\n",
       " 'überragen_flames',\n",
       " 'flames_herisau',\n",
       " 'herisau_bangen']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Take a look at the first few phrases of the first bigrammed article\n",
    "de_tx[0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the phrased text data to a csv file\n",
    "# Generate a list containing the phrased text data in form of strings in which all words and phrases are contained and separated by a blank (such that it's easy to read in later)\n",
    "de_tx_out = []\n",
    "for article in de_tx:\n",
    "    de_tx_out.append(\" \".join(article))\n",
    "\n",
    "# Create a correctly indexed dataframe containing the preprocessed data in a column and export it as a csv file\n",
    "pd.DataFrame(de_tx_out, index = de_idx.de_idx, columns = ['tx']).to_csv(\"Preprocessed/Phrased/de_phrased.csv\", index = True, encoding = 'utf-8-sig')"
   ]
  }
 ]
}