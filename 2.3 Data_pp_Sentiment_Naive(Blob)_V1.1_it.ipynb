{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd089a54e7346d33656f2940aba0e47a561becf96fe36c69c035d9bc66d085d8900",
   "display_name": "Python 3.7.9 64-bit ('Master_Thesis_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h1 style='margin:10px 5px'> \n",
    "Master Thesis Yannik Haller - Data Preprocessing for the Naïve Sentiment Classifier\n",
    "</h1>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "1. Load required packages and the data\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Hallk\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\pylab\\config.py:70: DeprecationWarning: InlineBackend._figure_formats_changed is deprecated in traitlets 4.1: use @observe and @unobserve instead.\n  def _figure_formats_changed(self, name, old, new):\n"
     ]
    }
   ],
   "source": [
    "# Import required baseline packages\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Change pandas' setting to print out long strings\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Spacy (for lemmatization)\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim (optional)\n",
    "import logging\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the appropriate working directory\n",
    "os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "\n",
    "# Read in the aggregated data\n",
    "it_tx = pd.read_csv(\"agg_csv_sparse_it.csv\", index_col = 0, dtype = {'so': object, 'la': object, 'tx': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(23621, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Take a look at the shape of the data\n",
    "it_tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the article IDs (i.e. index) of the language specific subsets\n",
    "it_idx = it_tx.index  # Italian"
   ]
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2. Preprocess the text data batchwise\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2.1 Define all required functions to preprocess the data (pre-cleaning, tokenizing, removing stop words and lemmatization)\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prerequisite\n",
    "\n",
    "# Run in terminal (cmd) after the appropriate environment has been activated (activate Master_Thesis_env)\n",
    "# German\n",
    "#python -m spacy download de_core_news_sm (check)\n",
    "#python -m spacy download de_core_news_md (check)\n",
    "#python -m spacy download de_core_news_lg (check)\n",
    "\n",
    "# English\n",
    "#python -m spacy download en_core_web_sm (check)\n",
    "#python -m spacy download en_core_web_md (check)\n",
    "#python -m spacy download en_core_web_lg (check) --> dir :  C:\\Users\\Hallk\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-tl3c3f5d\\wheels\\41\\75\\77\\c4a98e18b2c317a2a13931cbbea7e3ca7f3a21efc36adc1d71\n",
    "\n",
    "# French\n",
    "#python -m spacy download fr_core_news_sm (check)\n",
    "#python -m spacy download fr_core_news_md (check)\n",
    "#python -m spacy download fr_core_news_lg (check)\n",
    "\n",
    "# Italian\n",
    "#python -m spacy download it_core_news_sm (check)\n",
    "#python -m spacy download it_core_news_md (check)\n",
    "#python -m spacy download it_core_news_lg (check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define all required functions for the batchwise data preprocessing\n",
    "\n",
    "# Define a function to prepare/pre-clean the text data\n",
    "def pre_clean(articles):\n",
    "    # Raise an error if an inappropriate data type is given as an input\n",
    "    if(not isinstance(articles, list)):\n",
    "        raise ValueError(\"Invalid input type. Expected a list.\")\n",
    "\n",
    "    # Keep track of the processing time\n",
    "    t = time.time()\n",
    "    # Replace punctuations which are not followed by a blank with punctuations followed by a blank\n",
    "    articles = [re.sub(r'[\\.]', '. ', x) for x in articles]\n",
    "    # Separate words in which a lowercase letter is followed by a capital letter, since they usually do not belong together\n",
    "    articles = [re.sub('(^[a-z]*)+([A-Z])', r'\\1 \\2', x) for x in articles]\n",
    "    # Correct manually for those cases where a name like 'McDonalds' was separated to Mc Donalds\n",
    "    articles = [re.sub('Mc ', 'Mc', x) for x in articles]\n",
    "    # Replace quotation marks with a blank\n",
    "    articles = [re.sub('«', ' ', x) for x in articles]\n",
    "    articles = [re.sub('»', ' ', x) for x in articles]\n",
    "    # Remove percentage signs\n",
    "    articles = [re.sub('%', ' ', x) for x in articles]\n",
    "    # Remove distracting hyphens\n",
    "    articles = [re.sub(\"-\", \" \", x) for x in articles]\n",
    "    articles = [re.sub(\"–\", \" \", x) for x in articles]\n",
    "    # Replace new line characters (i.e. \\n) and multiple blanks with a single blank\n",
    "    articles = [re.sub('\\s+', ' ', x) for x in articles]\n",
    "    # Print out the processing time\n",
    "    print(\"Processing time for pre-cleaning: \", str((time.time() - t)/60), \"minutes\")\n",
    "\n",
    "    # Return the pre-cleaned text data\n",
    "    return articles\n",
    "\n",
    "\n",
    "# Define a function to perform the following tasks at once:\n",
    "## 1. Tokenize: transform text into a list of words, digits and punctuations\n",
    "## 2. Filter the tokens, such that only nouns, proper nouns, verbs, adjectives, adverbs and negations are kept, while digits and punctuations are removed\n",
    "## 3. Lemmatize: transform each word back to its word stem\n",
    "## 4. Lowercase the entire data\n",
    "def tokenize_filter_and_lemmatize(articles, nlp):\n",
    "    # Keep track of the processing time\n",
    "    t = time.time()\n",
    "    # Create a list to store the output\n",
    "    articles_out = []\n",
    "    # Define the list of allowed postags (pos = part of speech)\n",
    "    allowed_postags = ['PROPN', 'NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    # Define the list of allowed negations (for Italian)\n",
    "    allowed_negations = ['no', 'non', 'niente', 'nessuno']\n",
    "    # Create a loop to go through all articles in the input list of articles\n",
    "    for article in articles:\n",
    "        # Define the current article as the focal document\n",
    "        doc = nlp(article)\n",
    "        # Tokenize, filter and lemmatize the document, while filtering punctuations and unused word types (i.e. words which are not contained in the 'allowed_posttags' variable)\n",
    "        articles_out.append([token.lemma_.lower() for token in doc if (token.pos_ in allowed_postags) or (token.lemma_.lower() in allowed_negations)])\n",
    "    # Print out the processing time\n",
    "    print(\"Processing time for tokenizing, filtering and lemmatization: \", str((time.time() - t)/60), \"minutes\")\n",
    "    return articles_out"
   ]
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2.2 Define a function to preprocess and export the data batchwise\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the function to apply batchwise processing of the text data\n",
    "\n",
    "# Note: \"articles\" has to be a dataframe with a column tx containing the text files\n",
    "def process_batchwise(articles, language, batch_size = 100000, first_batch_number = 1):\n",
    "    # Raise an error if an inappropriate data type is given as an input\n",
    "    if(not isinstance(articles, pd.DataFrame)):\n",
    "        raise ValueError(\"Invalid input type. Expected a pandas DataFrame.\")\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "\n",
    "    # Get the number of batches\n",
    "    nbatches = int((len(articles)-1)/batch_size) + 1\n",
    "    # Store the index of the articles\n",
    "    idx = articles.index\n",
    "    # Convert the column of the dataframe that contains the articles to a list of articles, while overwriting the variable 'articles' to save RAM\n",
    "    articles = articles.tx.values.tolist()\n",
    "\n",
    "    # Initialize the appropriate spacy model depending on the language of the text data, while keeping only the tagger component (for efficiency)\n",
    "    if language == 'de':\n",
    "        nlp = spacy.load('de_core_news_lg', disable = ['tok2vec', 'morphologizer', 'senter', 'ner', 'attribute_ruler'])\n",
    "    elif language == 'en':\n",
    "        nlp = spacy.load('en_core_web_lg', disable = ['tok2vec', 'morphologizer', 'senter', 'ner', 'attribute_ruler'])\n",
    "    elif language == 'fr':\n",
    "        nlp = spacy.load('fr_core_news_lg', disable = ['tok2vec', 'morphologizer', 'senter', 'ner', 'attribute_ruler'])\n",
    "    elif language == 'it':\n",
    "        nlp = spacy.load('it_core_news_lg', disable = ['tok2vec', 'morphologizer', 'senter', 'ner', 'attribute_ruler'])\n",
    "    \n",
    "    # Set up a loop to process the data batchwise\n",
    "    for i in range(nbatches):\n",
    "        print('Processing batch #', i+first_batch_number, '...')\n",
    "        # Select the data related to the current batch\n",
    "        batch_min = batch_size * i\n",
    "        if i == (batch_size - 1):\n",
    "            batch_max = len(articles)\n",
    "        else:\n",
    "            batch_max = batch_size * (i+1)\n",
    "        batch_tx = articles[batch_min:batch_max]\n",
    "\n",
    "        # Pre-clean the data\n",
    "        batch_tx = pre_clean(batch_tx)\n",
    "        # Tokenize, filter and lemmatize the data\n",
    "        batch_tx = tokenize_filter_and_lemmatize(batch_tx, nlp)\n",
    "\n",
    "        ## Save the processed text data to a csv file\n",
    "        # Generate a list containing the preprocessed data in form of strings in which all lemmatized phrases are contained and separated by a blank (such that it's easy to read in later)\n",
    "        batch_tx_out = []\n",
    "        for article in batch_tx:\n",
    "            batch_tx_out.append(\" \".join(article))\n",
    "        # Create a correctly indexed dataframe containing the preprocessed data in a column and export it as a csv file\n",
    "        pd.DataFrame(batch_tx_out, index = idx[batch_min:batch_max], columns = ['tx']).to_csv(\n",
    "            \"Preprocessed/Sentiment_Analysis/Lemmatized/\"+language+\"_lemmatized_senti_batch_\"+str(i+first_batch_number)+\".csv\", index = True, encoding = 'utf-8-sig'\n",
    "        )\n",
    "\n",
    "        # Delete large unused variables to save memory\n",
    "        del batch_tx_out\n",
    "    print(\"DONE! ;)\")"
   ]
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2.3 Apply batchwise preprocessing and store the preprocessed data externally as csv files\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing batch # 1 ...\n",
      "Processing time for pre-cleaning:  0.06947460969289145 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  11.725524806976319 minutes\n",
      "DONE! ;)\n"
     ]
    }
   ],
   "source": [
    "# Apply batchwise preprocessing by means of the previously defined function\n",
    "process_batchwise(it_tx, language = 'it', batch_size = 100000, first_batch_number = 1)"
   ]
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "3. Read in and inspect the filtered and lemmatized data\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read in and concatenate the filtered and lemmatized data\n",
    "def read_lemmatized(language, tokenize = True):\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "\n",
    "    # Set the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data\\\\Preprocessed\\\\Sentiment_Analysis\\\\Lemmatized')\n",
    "\n",
    "    # Get a list of all files to read and concatenate\n",
    "    extension = 'csv'\n",
    "    all_filenames = [i for i in glob.glob(language+\"_lemmatized_senti_batch_*.{}\".format(extension))]\n",
    "    # Concatenate all files in the list to one dataframe\n",
    "    batches_aggregated = pd.concat([pd.read_csv(f, index_col = 0, dtype = {'tx': object}) for f in all_filenames])\n",
    "    # Get the articles' indices together with an enumeration to identify them in the list of filtered and lemmatized articles\n",
    "    idx = batches_aggregated.index\n",
    "    idx = pd.DataFrame(idx, columns = [language+'_idx'])\n",
    "    # Tokenize the data again if tokenize = True\n",
    "    if tokenize:\n",
    "        batches_aggregated = retokenize(batches_aggregated.tx.values.tolist())\n",
    "    else:\n",
    "        batches_aggregated = batches_aggregated.tx.values.tolist()\n",
    "    \n",
    "    # Reset the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data')  \n",
    "\n",
    "    # Return the precleaned data\n",
    "    return batches_aggregated, idx\n",
    "\n",
    "# Define a function to retokenize the filtered and lemmatized text data\n",
    "def retokenize(articles):\n",
    "    articles_out = []\n",
    "    for article in articles:\n",
    "        articles_out.append(article.split())\n",
    "    return articles_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the filtered and lemmatized data\n",
    "it_tx_lemm, it_idx = read_lemmatized('it', tokenize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        it_idx\n",
       "0       313578\n",
       "1       460527\n",
       "2       460528\n",
       "3       460529\n",
       "4       460530\n",
       "...        ...\n",
       "23616  2425111\n",
       "23617  2425112\n",
       "23618  2425113\n",
       "23619  2425114\n",
       "23620  2425115\n",
       "\n",
       "[23621 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>it_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>313578</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>460527</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>460528</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>460529</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>460530</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23616</th>\n      <td>2425111</td>\n    </tr>\n    <tr>\n      <th>23617</th>\n      <td>2425112</td>\n    </tr>\n    <tr>\n      <th>23618</th>\n      <td>2425113</td>\n    </tr>\n    <tr>\n      <th>23619</th>\n      <td>2425114</td>\n    </tr>\n    <tr>\n      <th>23620</th>\n      <td>2425115</td>\n    </tr>\n  </tbody>\n</table>\n<p>23621 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Take a look at the dataframe containing the according index\n",
    "it_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "200320"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Take a look at the size of the filtered and lemmatized data\n",
    "sys.getsizeof(it_tx_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['fermati', 'obbligare', 'oppresso', 'mondare', 'ginocchio', 'forse']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Take a look at the first few tokens of the first element of filtered and lemmatized data\n",
    "it_tx_lemm[0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Fermati obbligato a meditare. Hai oppresso il mondo in ginocchio, forse per pregare …Applaudivamo allo stadio. Ora acclamiamo dal balcone, dai ricchi del pallone a splendidi medici.Ci hai tolto la stretta di mano, la cosa più amicale, il bacio. Senza contatti un solo sfiorarsi, come un soffio di vento. Soli come un solco senza seme.A chiederci i mille perché.Convinti che la vita è breve, non va perso neanche un istante, potrebbe nascere la luna piena, in tutta la sua bellezza e verità.Ci torna felice, compiuto il canto: non camminerete mai da soli.Dubitiamo negli esperti del nulla. Lasciamo volare il pipistrello, maestro della biodiversità, senza colpe.Usciamo dall’inferno che toglie il respiro. Reagiamo al torrente della vita. Dissetiamoci alle fonti naturali, sempre convinti che l’amore ci  salverà, unica risposta e tutto vince.Felicità, serenità e si tornerà a  sorridere.Sicuri che Dio sconfiggerà anche il coronavirus.Rodolfo Fasani, Mesocco'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Compare it to the initial text\n",
    "it_tx.tx.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary variables to save RAM\n",
    "del it_tx_lemm"
   ]
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "4. Supplemental (manual) cleaning of the filtered and lemmatized data\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the filtered and lemmatized data (untokenized)\n",
    "it_tx, it_idx = read_lemmatized('it', tokenize = False) # Overwrite the variable containing the uncleaned data to save RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        it_idx\n",
       "0       313578\n",
       "1       460527\n",
       "2       460528\n",
       "3       460529\n",
       "4       460530\n",
       "...        ...\n",
       "23616  2425111\n",
       "23617  2425112\n",
       "23618  2425113\n",
       "23619  2425114\n",
       "23620  2425115\n",
       "\n",
       "[23621 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>it_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>313578</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>460527</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>460528</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>460529</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>460530</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23616</th>\n      <td>2425111</td>\n    </tr>\n    <tr>\n      <th>23617</th>\n      <td>2425112</td>\n    </tr>\n    <tr>\n      <th>23618</th>\n      <td>2425113</td>\n    </tr>\n    <tr>\n      <th>23619</th>\n      <td>2425114</td>\n    </tr>\n    <tr>\n      <th>23620</th>\n      <td>2425115</td>\n    </tr>\n  </tbody>\n</table>\n<p>23621 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Take a look at the dataframe containing the according index\n",
    "it_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'fermati obbligare oppresso mondare ginocchio forse applaudivamo stadio ora acclamare balcone ricco pallone splendido medico togliere stringere mano cosa molto baciare contatto solere sfiorarsi soffiare vento soli solcare seme chiederci perché convinti vita essere non perdere neanche istante lunare pieno bellezza verità tornire compiere cantare non camminare mai solo dubitiamo esperto nullo lasciamo pipistrello maestro biodiversità colpa usciamo inferno togliere respirare reagiamo torrente vita dissetiamoci fonte naturale sempre convinto amore salvare unico rispondere vincere felicità serenità tornare sicuri dio sconfiggere anche coronavirus rodolfo fasani mesocco'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Take a look at the first element of the (untokenized) filtered and lemmatized data\n",
    "it_tx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the supplemental/manual cleaning to the filtered and lemmatized data\n",
    "def supp_clean(articles):\n",
    "    # Raise an error if an inappropriate data type is given as an input\n",
    "    if(not isinstance(articles, list)):\n",
    "        raise ValueError(\"Invalid input type. Expected a list.\")\n",
    "\n",
    "    # Keep track of the processing time\n",
    "    t = time.time()\n",
    "    # Remove any links starting with http:// or https://\n",
    "    articles = [re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+').sub('',x) for x in articles]\n",
    "    # Remove any links starting with www.\n",
    "    articles = [re.compile('www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+').sub('',x) for x in articles]\n",
    "    # Remove any instances where 1 to 3 initiating letters are followed by a dot (either once at the end or after each letter), since such cases usually represent abbreviations with low semantic meaning\n",
    "    articles = [re.compile(' [a-z][\\.]?[a-z]?[\\.]?[a-z]?\\.+').sub(' ', x) for x in articles]\n",
    "    # Remove any remaining digit\n",
    "    articles = [re.sub(r'\\b\\d+\\b', '', x) for x in articles]\n",
    "    # Remove anything except words, spaces and the & sign, since this might appear in certain names\n",
    "    articles = [re.sub(r'[^\\w\\s\\&]','', x) for x in articles]\n",
    "    # Remove a list of specific words which appear quite often but do not seem to add any semantic value (and the auxiliary verb essere, as this is not identified as such by the Italian Spacy module)\n",
    "    words_to_remove = ['awp','afp','essere']\n",
    "    for word in words_to_remove:\n",
    "        articles = [re.sub(' '+word, '', x) for x in articles] \n",
    "    # Replace new line characters (i.e. \\n) and multiple blanks with a single blank\n",
    "    articles = [re.sub('\\s+', ' ', x) for x in articles]\n",
    "    # Print out the processing time\n",
    "    print(\"Processing time for supplemental manual cleaning: \", str((time.time() - t)/60), \"minutes\")\n",
    "\n",
    "    # Return the manually cleaned text data\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing time for supplemental manual cleaning:  0.04911261002222697 minutes\n"
     ]
    }
   ],
   "source": [
    "# Apply the supplemental/manual cleaning by means of the previously defined function\n",
    "it_tx = supp_clean(it_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'fermati obbligare oppresso mondare ginocchio forse applaudivamo stadio ora acclamare balcone ricco pallone splendido medico togliere stringere mano cosa molto baciare contatto solere sfiorarsi soffiare vento soli solcare seme chiederci perché convinti vita non perdere neanche istante lunare pieno bellezza verità tornire compiere cantare non camminare mai solo dubitiamo esperto nullo lasciamo pipistrello maestro biodiversità colpa usciamo inferno togliere respirare reagiamo torrente vita dissetiamoci fonte naturale sempre convinto amore salvare unico rispondere vincere felicità serenità tornare sicuri dio sconfiggere anche coronavirus rodolfo fasani mesocco'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Take a look at the first element of the fully preprocessed data\n",
    "it_tx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compart the fully preprocessed data to the initial text (copy paste from above)"
   ]
  },
  {
   "source": [
    "'Fermati obbligato a meditare. Hai oppresso il mondo in ginocchio, forse per pregare …Applaudivamo allo stadio. Ora acclamiamo dal balcone, dai ricchi del pallone a splendidi medici.Ci hai tolto la stretta di mano, la cosa più amicale, il bacio. Senza contatti un solo sfiorarsi, come un soffio di vento. Soli come un solco senza seme.A chiederci i mille perché.Convinti che la vita è breve, non va perso neanche un istante, potrebbe nascere la luna piena, in tutta la sua bellezza e verità.Ci torna felice, compiuto il canto: non camminerete mai da soli.Dubitiamo negli esperti del nulla. Lasciamo volare il pipistrello, maestro della biodiversità, senza colpe.Usciamo dall’inferno che toglie il respiro. Reagiamo al torrente della vita. Dissetiamoci alle fonti naturali, sempre convinti che l’amore ci  salverà, unica risposta e tutto vince.Felicità, serenità e si tornerà a  sorridere.Sicuri che Dio sconfiggerà anche il coronavirus.Rodolfo Fasani, Mesocco'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "5. Export the fully preprocessed data as one csv file\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to export the fully preprocessed data\n",
    "def export_preprocessed(language, articles, idx, data_tokenized = True):\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "\n",
    "    # Set the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "\n",
    "    # Untokenize the data if it is still tokenized\n",
    "    if data_tokenized:\n",
    "        # Generate a list containing the fully preprocessed data in form of strings in which all precleaned unigrams are contained and separated by a blank (such that it's easy to read in later)\n",
    "        articles_out = []\n",
    "        for article in articles:\n",
    "            articles_out.append(\" \".join(article))\n",
    "        # Overwrite the variable which stores the tokenized articles\n",
    "        articles = articles_out\n",
    "        # Delete the variable articles_out to save RAM\n",
    "        del articles_out\n",
    "    \n",
    "    # Create a correctly indexed dataframe containing the fully preprocessed data in a column and export it as a csv file\n",
    "    pd.DataFrame(articles, index = idx, columns = ['tx']).to_csv(\"Preprocessed/Sentiment_Analysis/\"+language+\"_preprocessed_senti.csv\", index = True, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the fully preprocessed data\n",
    "export_preprocessed('it', it_tx, it_idx.it_idx.to_list(), False)"
   ]
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "6. Read in the fully preprocessed data\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read in the fully preprocessed data\n",
    "def read_preprocessed(language, tokenize = True):\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "    \n",
    "    # Set the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "\n",
    "    # Define the name of the file to load\n",
    "    filename = \"Preprocessed/Sentiment_Analysis/\"+language+\"_preprocessed_senti.csv\"\n",
    "\n",
    "    # Read in the dataframe containing the text data\n",
    "    tx_pp = pd.read_csv(filename, index_col = 0, dtype = {'tx': object})\n",
    "\n",
    "    # Get the articles' index together with an enumeration to identify their position in the list of precleaned articles\n",
    "    idx = tx_pp.index\n",
    "    idx = pd.DataFrame(idx, columns = [language+'_idx'])\n",
    "\n",
    "    # Reduce the dataframe to a list containing the text data\n",
    "    tx_pp = tx_pp.tx.to_list()\n",
    "\n",
    "    # Tokenize the data again if tokenize = True (RAM-saving)\n",
    "    if tokenize:\n",
    "        tx_pp = retokenize(tx_pp)\n",
    "\n",
    "    # Return the preprocessed data\n",
    "    return tx_pp, idx\n",
    "\n",
    "# Define a function to retokenize the preprocessed text data (RAM-saving)\n",
    "def retokenize(article_list):\n",
    "    for i in range(len(article_list)):\n",
    "        temp_tx = str(article_list[i]).split()\n",
    "        article_list[i] = temp_tx\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the fully preprocessed data\n",
    "it_tx, it_idx = read_preprocessed('it', tokenize = True) # Overwrite the variables used above to save RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        it_idx\n",
       "0       313578\n",
       "1       460527\n",
       "2       460528\n",
       "3       460529\n",
       "4       460530\n",
       "...        ...\n",
       "23616  2425111\n",
       "23617  2425112\n",
       "23618  2425113\n",
       "23619  2425114\n",
       "23620  2425115\n",
       "\n",
       "[23621 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>it_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>313578</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>460527</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>460528</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>460529</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>460530</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23616</th>\n      <td>2425111</td>\n    </tr>\n    <tr>\n      <th>23617</th>\n      <td>2425112</td>\n    </tr>\n    <tr>\n      <th>23618</th>\n      <td>2425113</td>\n    </tr>\n    <tr>\n      <th>23619</th>\n      <td>2425114</td>\n    </tr>\n    <tr>\n      <th>23620</th>\n      <td>2425115</td>\n    </tr>\n  </tbody>\n</table>\n<p>23621 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Take a look at the dataframe containing the according index\n",
    "it_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['fermati',\n",
       " 'obbligare',\n",
       " 'oppresso',\n",
       " 'mondare',\n",
       " 'ginocchio',\n",
       " 'forse',\n",
       " 'applaudivamo',\n",
       " 'stadio',\n",
       " 'ora',\n",
       " 'acclamare',\n",
       " 'balcone',\n",
       " 'ricco',\n",
       " 'pallone',\n",
       " 'splendido',\n",
       " 'medico',\n",
       " 'togliere',\n",
       " 'stringere',\n",
       " 'mano',\n",
       " 'cosa',\n",
       " 'molto',\n",
       " 'baciare',\n",
       " 'contatto',\n",
       " 'solere',\n",
       " 'sfiorarsi',\n",
       " 'soffiare',\n",
       " 'vento',\n",
       " 'soli',\n",
       " 'solcare',\n",
       " 'seme',\n",
       " 'chiederci',\n",
       " 'perché',\n",
       " 'convinti',\n",
       " 'vita',\n",
       " 'non',\n",
       " 'perdere',\n",
       " 'neanche',\n",
       " 'istante',\n",
       " 'lunare',\n",
       " 'pieno',\n",
       " 'bellezza',\n",
       " 'verità',\n",
       " 'tornire',\n",
       " 'compiere',\n",
       " 'cantare',\n",
       " 'non',\n",
       " 'camminare',\n",
       " 'mai',\n",
       " 'solo',\n",
       " 'dubitiamo',\n",
       " 'esperto',\n",
       " 'nullo',\n",
       " 'lasciamo',\n",
       " 'pipistrello',\n",
       " 'maestro',\n",
       " 'biodiversità',\n",
       " 'colpa',\n",
       " 'usciamo',\n",
       " 'inferno',\n",
       " 'togliere',\n",
       " 'respirare',\n",
       " 'reagiamo',\n",
       " 'torrente',\n",
       " 'vita',\n",
       " 'dissetiamoci',\n",
       " 'fonte',\n",
       " 'naturale',\n",
       " 'sempre',\n",
       " 'convinto',\n",
       " 'amore',\n",
       " 'salvare',\n",
       " 'unico',\n",
       " 'rispondere',\n",
       " 'vincere',\n",
       " 'felicità',\n",
       " 'serenità',\n",
       " 'tornare',\n",
       " 'sicuri',\n",
       " 'dio',\n",
       " 'sconfiggere',\n",
       " 'anche',\n",
       " 'coronavirus',\n",
       " 'rodolfo',\n",
       " 'fasani',\n",
       " 'mesocco']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Take a look at the first element of the fully preprocessed and tokenized data\n",
    "it_tx[0]"
   ]
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "7. Quantitative summary of the data cleaning\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of words contained after data cleaning: 2999296\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of words contained after the data cleaning\n",
    "nwords_after = 0\n",
    "for article in it_tx:\n",
    "    nwords_after = nwords_after + len(article)\n",
    "print('Total number of words contained after data cleaning:', nwords_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average number of words per article after data cleaning: 126.97582659497904\n"
     ]
    }
   ],
   "source": [
    "# Get the average number of words per article after the data cleaning\n",
    "avg_nwords_after = nwords_after/len(it_tx)\n",
    "avg_nwords_after\n",
    "print('Average number of words per article after data cleaning:', avg_nwords_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary variables to save RAM\n",
    "del it_tx, it_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the uncleaned data\n",
    "os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "it_tx_uncleaned = pd.read_csv(\"agg_csv_sparse_it.csv\", index_col = 0, dtype = {'so': object, 'la': object, 'tx': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing time for pre-cleaning:  0.06998445590337117 minutes\n",
      "Total number of words contained before data cleaning: 5984264\n"
     ]
    }
   ],
   "source": [
    "## Count the total number of words contained before the data cleaning\n",
    "# Note: to get an appropriate count of the distinct words we must at least apply the very first low-level precleaning, to ensure that all words are separated properly and distracting signs are removed\n",
    "it_tx_uncleaned = pre_clean(it_tx_uncleaned.tx.tolist())\n",
    "# Count the total number of words\n",
    "nwords_before = 0\n",
    "for article in it_tx_uncleaned:\n",
    "    nwords_before = nwords_before + len(article.split())\n",
    "print('Total number of words contained before data cleaning:', nwords_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average number of words per article before data cleaning: 253.3450742982939\n"
     ]
    }
   ],
   "source": [
    "# Get the average number of words per article before the data cleaning\n",
    "avg_nwords_before = nwords_before/len(it_tx_uncleaned)\n",
    "print('Average number of words per article before data cleaning:', avg_nwords_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of words removed by the data cleaning: 2984968\n"
     ]
    }
   ],
   "source": [
    "# Get the number of removed words\n",
    "nwords_rm = nwords_before - nwords_after\n",
    "print('Number of words removed by the data cleaning:', nwords_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "49.8803 percent of the words have been removed by the data cleaning\n"
     ]
    }
   ],
   "source": [
    "# Get the ratio of the words that have been removed\n",
    "ratio_removed = nwords_rm / nwords_before\n",
    "print(np.round(ratio_removed*100,4),'percent of the words have been removed by the data cleaning')"
   ]
  }
 ]
}