{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h1 style='margin:10px 5px'> \n",
    "Master Thesis Yannik Haller - Data Preprocessing TextBlob\n",
    "</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "1. Load required packages and the data\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required baseline packages\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Change pandas' setting to print out long strings\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "# Spacy (for lemmatization)\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the appropriate working directory\n",
    "os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "\n",
    "# Read in the aggregated data\n",
    "de_tx = pd.read_csv(\"agg_csv_sparse_de.csv\", index_col = 0, dtype = {'so': object, 'la': object, 'tx': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1934313, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the shape of the data\n",
    "de_tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the article IDs (i.e. index) of the language specific subsets\n",
    "de_idx = de_tx.index  # German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2. Preprocess the text data batchwise\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2.1 Define all required functions to preprocess the data (pre-cleaning, tokenizing, removing stop words and lemmatization)\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define all required functions for the batchwise data preprocessing\n",
    "\n",
    "# Define a function to prepare/pre-clean the text data\n",
    "def pre_clean(articles):\n",
    "    # Raise an error if an inappropriate data type is given as an input\n",
    "    if(not isinstance(articles, list)):\n",
    "        raise ValueError(\"Invalid input type. Expected a list.\")\n",
    "\n",
    "    # Keep track of the processing time\n",
    "    t = time.time()\n",
    "\n",
    "    # Remove any links starting with http:// or https://\n",
    "    articles = [re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+').sub('',x) for x in articles]\n",
    "    # Remove any links starting with www.\n",
    "    articles = [re.compile('www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+').sub('',x) for x in articles]\n",
    "    \n",
    "    # Replace punctuations which are not followed by a blank with punctuations followed by a blank\n",
    "    articles = [re.sub(r'[\\.]', '. ', x) for x in articles]\n",
    "    # Separate words in which a lowercase letter is followed by a capital letter, since they usually do not belong together\n",
    "    articles = [re.sub('(^[a-z]*)+([A-Z])', r'\\1 \\2', x) for x in articles]\n",
    "    # Correct manually for those cases where a name like 'McDonalds' was separated to Mc Donalds\n",
    "    articles = [re.sub('Mc ', 'Mc', x) for x in articles]\n",
    "    # Replace quotation marks with a blank\n",
    "    articles = [re.sub('«', ' ', x) for x in articles]\n",
    "    articles = [re.sub('»', ' ', x) for x in articles]\n",
    "    # Remove percentage signs\n",
    "    articles = [re.sub('%', ' ', x) for x in articles]\n",
    "    # Remove distracting hyphens\n",
    "    articles = [re.sub(\"-\", \" \", x) for x in articles]\n",
    "    articles = [re.sub(\"–\", \" \", x) for x in articles]\n",
    "    # Replace control characters (e.g. \\n or \\t) and multiple blanks with a single blank\n",
    "    articles = [re.sub('\\s+', ' ', x) for x in articles]\n",
    "    # Print out the processing time\n",
    "    print(\"Processing time for pre-cleaning: \", str((time.time() - t)/60), \"minutes\")\n",
    "\n",
    "    # Return the pre-cleaned text data\n",
    "    return articles\n",
    "\n",
    "\n",
    "# Define a function to perform the following tasks at once:\n",
    "## 1. Tokenize: transform text into a list of words, digits and punctuations\n",
    "## 2. Filter the tokens, such that only nouns, proper nouns, verbs, adjectives, adverbs and negations are kept, while digits and punctuations are removed\n",
    "## 3. Lemmatize: transform each word back to its word stem\n",
    "## 4. Lowercase the entire data\n",
    "def tokenize_filter_and_lemmatize(articles, nlp):\n",
    "    # Keep track of the processing time\n",
    "    t = time.time()\n",
    "    # Create a list to store the output\n",
    "    articles_out = []\n",
    "    # Define the list of allowed postags (pos = part of speech)\n",
    "    allowed_postags = ['PROPN', 'NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    # Define the list of allowed negations (for German)\n",
    "    allowed_negations = ['nie', 'nichts', 'nicht', 'kein', 'wenig', 'ohne']\n",
    "    # Create a loop to go through all articles in the input list of articles\n",
    "    for article in articles:\n",
    "        # Define the current article as the focal document\n",
    "        doc = nlp(article)\n",
    "        # Tokenize, filter and lemmatize the document, while filtering out punctuations and unused word types\n",
    "        articles_out.append([token.lemma_.lower() for token in doc if (token.pos_ in allowed_postags) or (token.lemma_.lower() in allowed_negations)])\n",
    "    # Print out the processing time\n",
    "    print(\"Processing time for tokenizing, filtering and lemmatization: \", str((time.time() - t)/60), \"minutes\")\n",
    "    return articles_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2.2 Define a function to preprocess and export the data batchwise\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the function to apply batchwise processing of the text data\n",
    "\n",
    "# Note: \"articles\" has to be a dataframe with a column tx containing the text files\n",
    "def process_batchwise(articles, language, batch_size = 100000, first_batch_number = 1):\n",
    "    # Raise an error if an inappropriate data type is given as an input\n",
    "    if(not isinstance(articles, pd.DataFrame)):\n",
    "        raise ValueError(\"Invalid input type. Expected a pandas DataFrame.\")\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "\n",
    "    # Get the number of batches\n",
    "    nbatches = int((len(articles)-1)/batch_size) + 1\n",
    "    # Store the index of the articles\n",
    "    idx = articles.index\n",
    "    # Convert the column of the dataframe that contains the articles to a list of articles, while overwriting the variable 'articles' to save RAM\n",
    "    articles = articles.tx.values.tolist()\n",
    "\n",
    "    # Initialize the appropriate spacy model depending on the language of the text data, while keeping only the tagger component (for efficiency)\n",
    "    if language == 'de':\n",
    "        nlp = spacy.load('de_core_news_lg', disable = ['tok2vec', 'morphologizer', 'senter', 'ner', 'attribute_ruler'])\n",
    "    elif language == 'en':\n",
    "        nlp = spacy.load('en_core_web_lg', disable = ['tok2vec', 'morphologizer', 'senter', 'ner', 'attribute_ruler'])\n",
    "    elif language == 'fr':\n",
    "        nlp = spacy.load('fr_core_news_lg', disable = ['tok2vec', 'morphologizer', 'senter', 'ner', 'attribute_ruler'])\n",
    "    elif language == 'it':\n",
    "        nlp = spacy.load('it_core_news_lg', disable = ['tok2vec', 'morphologizer', 'senter', 'ner', 'attribute_ruler'])\n",
    "    \n",
    "    # Set up a loop to process the data batchwise\n",
    "    for i in range(nbatches):\n",
    "        print('Processing batch #', i+first_batch_number, '...')\n",
    "        # Select the data related to the current batch\n",
    "        batch_min = batch_size * i\n",
    "        if i == (nbatches - 1):\n",
    "            batch_max = len(articles)\n",
    "        else:\n",
    "            batch_max = batch_size * (i+1)\n",
    "        batch_tx = articles[batch_min:batch_max]\n",
    "\n",
    "        # Pre-clean the data\n",
    "        batch_tx = pre_clean(batch_tx)\n",
    "        # Tokenize, filter and lemmatize the data\n",
    "        batch_tx = tokenize_filter_and_lemmatize(batch_tx, nlp)\n",
    "\n",
    "        ## Save the processed text data to a csv file\n",
    "        # Generate a list containing the preprocessed data in form of strings in which all lemmatized phrases are contained and separated by a blank (such that it's easy to read in later)\n",
    "        batch_tx_out = []\n",
    "        for article in batch_tx:\n",
    "            batch_tx_out.append(\" \".join(article))\n",
    "        # Create a correctly indexed dataframe containing the preprocessed data in a column and export it as a csv file\n",
    "        pd.DataFrame(batch_tx_out, index = idx[batch_min:batch_max], columns = ['tx']).to_csv(\n",
    "            \"Preprocessed/Sentiment_Analysis/Lemmatized/\"+language+\"_lemmatized_senti_batch_\"+str(i+first_batch_number)+\".csv\", index = True, encoding = 'utf-8-sig'\n",
    "        )\n",
    "\n",
    "        # Delete large unused variables to save memory\n",
    "        del batch_tx_out\n",
    "    print(\"DONE! ;)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2.3 Apply batchwise preprocessing and store the preprocessed data externally as csv files\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch # 1 ...\n",
      "Processing time for pre-cleaning:  0.4581407109896342 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  68.1004353483518 minutes\n",
      "Processing batch # 2 ...\n",
      "Processing time for pre-cleaning:  0.4717169920603434 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  65.81250176032384 minutes\n",
      "Processing batch # 3 ...\n",
      "Processing time for pre-cleaning:  0.4857803265253703 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  72.64777606725693 minutes\n",
      "Processing batch # 4 ...\n",
      "Processing time for pre-cleaning:  0.4697477459907532 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  81.45349471966425 minutes\n",
      "Processing batch # 5 ...\n",
      "Processing time for pre-cleaning:  0.433683451016744 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  72.35946209828059 minutes\n",
      "DONE! ;)\n"
     ]
    }
   ],
   "source": [
    "# Apply batchwise preprocessing by means of the previously defined function\n",
    "# Batches 1-5\n",
    "process_batchwise(de_tx[:500000], language = 'de', batch_size = 100000, first_batch_number = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch # 6 ...\n",
      "Processing time for pre-cleaning:  0.540809675057729 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  81.53184503316879 minutes\n",
      "Processing batch # 7 ...\n",
      "Processing time for pre-cleaning:  0.6536357482274373 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  95.98633125623067 minutes\n",
      "Processing batch # 8 ...\n",
      "Processing time for pre-cleaning:  0.7139591574668884 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  110.23240004380544 minutes\n",
      "Processing batch # 9 ...\n",
      "Processing time for pre-cleaning:  0.6416012843449911 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  106.77491083542506 minutes\n",
      "Processing batch # 10 ...\n",
      "Processing time for pre-cleaning:  0.4863160332043966 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  73.21589245398839 minutes\n",
      "DONE! ;)\n"
     ]
    }
   ],
   "source": [
    "# Batches 6-10\n",
    "process_batchwise(de_tx[500000:1000000], language = 'de', batch_size = 100000, first_batch_number = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch # 11 ...\n",
      "Processing time for pre-cleaning:  0.4868136405944824 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  71.40801916122436 minutes\n",
      "Processing batch # 12 ...\n",
      "Processing time for pre-cleaning:  0.5520401914914449 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  79.00566128094991 minutes\n",
      "Processing batch # 13 ...\n",
      "Processing time for pre-cleaning:  0.4907376249631246 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  69.40921380917231 minutes\n",
      "Processing batch # 14 ...\n",
      "Processing time for pre-cleaning:  0.5029875238736471 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  68.68988905350368 minutes\n",
      "Processing batch # 15 ...\n",
      "Processing time for pre-cleaning:  0.4651066025098165 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  68.85461974938711 minutes\n",
      "DONE! ;)\n"
     ]
    }
   ],
   "source": [
    "# Batches 11-15\n",
    "process_batchwise(de_tx[1000000:1500000], language = 'de', batch_size = 100000, first_batch_number = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch # 16 ...\n",
      "Processing time for pre-cleaning:  0.43460333744684854 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  62.765749108791354 minutes\n",
      "Processing batch # 17 ...\n",
      "Processing time for pre-cleaning:  0.4038864612579346 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  56.21139123042425 minutes\n",
      "Processing batch # 18 ...\n",
      "Processing time for pre-cleaning:  0.6051807125409444 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  95.92116247415542 minutes\n",
      "Processing batch # 19 ...\n",
      "Processing time for pre-cleaning:  0.6003604650497436 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  88.91763996283213 minutes\n",
      "Processing batch # 20 ...\n",
      "Processing time for pre-cleaning:  0.15742949644724527 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  21.651685535907745 minutes\n",
      "DONE! ;)\n"
     ]
    }
   ],
   "source": [
    "# Batches 16-20\n",
    "process_batchwise(de_tx[1500000:], language = 'de', batch_size = 100000, first_batch_number = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "3. Read in and inspect the filtered and lemmatized data\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read in and concatenate the filtered and lemmatized data\n",
    "def read_lemmatized(language, tokenize = True):\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "\n",
    "    # Set the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data\\\\Preprocessed\\\\Sentiment_Analysis\\\\Lemmatized')\n",
    "\n",
    "    # Get a list of all files to read and concatenate\n",
    "    extension = 'csv'\n",
    "    all_filenames = [i for i in glob.glob(language+\"_lemmatized_senti_batch_*.{}\".format(extension))]\n",
    "    # Concatenate all files in the list to one dataframe\n",
    "    batches_aggregated = pd.concat([pd.read_csv(f, index_col = 0, dtype = {'tx': object}) for f in all_filenames])\n",
    "    # Get the articles' indices together with an enumeration to identify them in the list of filtered and lemmatized articles\n",
    "    idx = batches_aggregated.index\n",
    "    idx = pd.DataFrame(idx, columns = [language+'_idx'])\n",
    "    # Tokenize the data again if tokenize = True\n",
    "    if tokenize:\n",
    "        batches_aggregated = retokenize(batches_aggregated.tx.values.tolist())\n",
    "    else:\n",
    "        batches_aggregated = batches_aggregated.tx.values.tolist()\n",
    "    \n",
    "    # Reset the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data')  \n",
    "\n",
    "    # Return the precleaned data\n",
    "    return batches_aggregated, idx\n",
    "\n",
    "# Define a function to retokenize the filtered and lemmatized text data\n",
    "def retokenize(articles):\n",
    "    articles_out = []\n",
    "    for article in articles:\n",
    "        articles_out.append(article.split())\n",
    "    return articles_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the filtered and lemmatized data\n",
    "de_tx_lemm, de_idx = read_lemmatized('de', tokenize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934308</th>\n",
       "      <td>2441178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934309</th>\n",
       "      <td>2441179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934310</th>\n",
       "      <td>2441180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934311</th>\n",
       "      <td>2441181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934312</th>\n",
       "      <td>2441182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1934313 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          de_idx\n",
       "0          16553\n",
       "1          16554\n",
       "2          16555\n",
       "3          16556\n",
       "4          16557\n",
       "...          ...\n",
       "1934308  2441178\n",
       "1934309  2441179\n",
       "1934310  2441180\n",
       "1934311  2441181\n",
       "1934312  2441182\n",
       "\n",
       "[1934313 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the dataframe containing the according index\n",
    "de_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15673408"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the size of the filtered and lematized data\n",
    "sys.getsizeof(de_tx_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rückkehrer', 'stefan', 'meier', 'überragen', '7:6', 'flames']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first few tokens of the first element of filtered and lemmatized data\n",
    "de_tx_lemm[0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rückkehrer Stefan Meier überragt beim 7:6 gegen die Flames. Herisau bangt allerdings am Schluss.Lukas PfiffnerIn der vergangenen Saison tat sich der UHC Herisau darin hervor, immer wieder einen Rückstand aufzuholen und Partien zu kehren. In der noch jungen 1.-Liga-Meisterschaft 2020/21 lebt das Team mindestens in den Heimspielen einem neuen Trend nach: trotz deutlicher Führung noch zu zittern.Am Samstag lagen die überzeugenden Ausserrhoder 2:0 vorne, sie reagierten auf den Ausgleich der Flames mit drei Toren innert dreier Minuten. Sie besassen mit Stefan Meier, der während neun Saisons für Wasa verteidigt hat und im Sommer aus der NLA zu seinem Stammverein zurückgekehrt ist, einen herausragenden Stürmer. Elf Minuten vor der Sirene hiess es 6:3, zum fünften Mal hatte Meier seinen Stock im Spiel. Mit der Sicherheit am Ball ging allerdings auch die Führung verloren. Der komplette Zusammenbruch drohte. Die Flames konnten aber die Gewichte nicht total verschieben – und ein eindrücklicher Effort von Niklas Hess trug den Gastgebern den Sieg ein (57.).Der Trainer sagte: «Grosses Kino»Schon eine Woche zuvor hatte Meier bei der einzigen Herisauer Niederlage (5:7 gegen Pfannenstiel Egg) drei Treffer erzielt. Er liegt mit sechs Toren und vier Assists nun auf Platz fünf der Skorer in der Gruppe 2. Meier bewegt sich – gemessen an 190\\xa0cm Grösse und 82 kg Gewicht – erstaunlich geschmeidig, er kann sich am Ball behaupten und weist einen wuchtigen Schuss auf. «Wenn es so läuft wie aktuell, ist es vorne natürlich schön», meinte der 29-Jährige. Er harmonierte mit seinen Linienpartnern Joel Conzett und Silas Stucki vorzüglich. «Beide sind kreativ.» Trainer Nico Raschle sprach von «grossem Kino», was Meiers Auftritt am Samstag und allgemein seine Einstellung betreffe: Er sei nicht in die 1. Liga gekommen, um seine Karriere einfach ein wenig «ausplämperlen» zu lassen. Dies bestätigt Meier indirekt. «Es war ganz gut, eine neue Position zugeteilt zu bekommen.» Da könne man sich nochmals richtig «reinhängen». Warum spielt Meier in Herisau vorne? «Weil wir uns von ihm das erhoffen, was ihm heute gelungen ist», sagte Raschle. Im resultatmässigen Notstand muss man auch einmal einen Entscheid rückgängig machen: Die Flames hatten innert 90 Sekunden drei Tore geschossen, der Trainer nahm ein Time-out und beorderte Meier für die letzten vier Minuten in die Verteidigung zurück. Ein Problem sei die kurzfristige Umstellung nicht gewesen, meinte dieser. «In der Abwehr machst du vieles über die Routine und das Positionsspiel.» Er habe zudem jahrelang mit Torhüter Dominic Jud im Rücken verteidigt. «Ich kenne seine Anweisungen genau.»Am Sonntagabend CupeinsatzFür Herisau brachte der Samstag im vierten Spiel den dritten Erfolg. Es belegt den zweiten Platz (2,25 Punkte pro Partie) hinter Bassersdorf Nürensdorf (2,5). Seit dieser Saison werden nicht mehr die effektiven Punkte für die Erstellung der Tabelle berücksichtigt, sondern die Quotienten. Bisher gab es in der Gruppe 2 trotz Corona allerdings noch keine Spielabsagen. Am Sonntagabend traten die Ausserrhoder beim 2.-Ligisten Grabs-Werdenberg zum Cupspiel an.Herisau – Flames 7:6 (1:0, 1:2, 5:4)Sportzentrum. – 98 Zuschauer. – Sr. Cereda/Locatelli. Tore: 15. S. Meier (Penalty) 1:0. 22. S. Meier (S. Stucki) 2:0. 25. Mattsson (Bernet) 2:1. 37. Liechti (Jenny) 2:2. 42. Conzett (S. Meier) 3:2. 44. (43:47) Conzett (S. Meier) 4:2. 45. (44:31) Germann (Brandes) 5:2. 49. (48:21) Swoboda 5:3. 49. (48:59) S. Meier (S. Stucki) 6:3. 55. (54:10) B. Jud (Mattsson, Ausschluss Schilling) 6:4. 55. (54:39) Dürr (J. Jud) 6:5. 56. (55:40) Mattsson (Rautio) 6:6. 57. (56:44) Hess 7:6.Herisau: D. Jud; Brunner, Schwarz; Rüegg, Schmid; Stern, L. Stucki; Schweizer; Hess, Schilling, Sandmeier; S. Stucki, Conzett, S. Meier; Germann, Mittelholzer, Wetter; Brandes. Strafen: je 1-mal 2 Minuten.Konzentration bis am Schluss: Herisaus Torhüter Dominic Jud sieht einen Schuss auf sich zukommen. Bild: Lukas Pfiffner'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare it to the initial text\n",
    "de_tx.tx.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rückgängig',\n",
       " 'machen',\n",
       " 'flames',\n",
       " 'innert',\n",
       " 'sekunde',\n",
       " 'tor',\n",
       " 'schießen',\n",
       " 'trainer',\n",
       " 'nehmen',\n",
       " 'beordern',\n",
       " 'meier',\n",
       " 'letzt',\n",
       " 'minute',\n",
       " 'verteidigung',\n",
       " 'problem',\n",
       " 'kurzfristig',\n",
       " 'umstellung',\n",
       " 'nicht',\n",
       " 'meinen',\n",
       " 'abwehr',\n",
       " 'machen',\n",
       " 'routine',\n",
       " 'positionsspiel',\n",
       " 'zudem',\n",
       " 'jahrelang',\n",
       " 'torhüter',\n",
       " 'dominic',\n",
       " 'jud',\n",
       " 'rücken',\n",
       " 'verteidigen',\n",
       " 'kennen',\n",
       " 'anweisung',\n",
       " 'genau',\n",
       " 'sonntagabend',\n",
       " 'cupeinsatzfür',\n",
       " 'herisau',\n",
       " 'bringen',\n",
       " 'samstag',\n",
       " 'viert',\n",
       " 'spiel',\n",
       " 'dritt',\n",
       " 'erfolg',\n",
       " 'belegen',\n",
       " 'zweit',\n",
       " 'platz',\n",
       " 'punkt',\n",
       " 'partie',\n",
       " 'bassersdorf',\n",
       " 'nürensdorf',\n",
       " 'saison',\n",
       " 'nicht',\n",
       " 'mehr',\n",
       " 'effektiv',\n",
       " 'punkt',\n",
       " 'erstellung',\n",
       " 'tabelle',\n",
       " 'berücksichtigen',\n",
       " 'quotient',\n",
       " 'bisher',\n",
       " 'geben',\n",
       " 'gruppe',\n",
       " 'corona',\n",
       " 'allerdings',\n",
       " 'noch',\n",
       " 'kein',\n",
       " 'spielabsagen',\n",
       " 'sonntagabend',\n",
       " 'treten',\n",
       " 'ausserrhoder',\n",
       " '2.',\n",
       " 'ligisten',\n",
       " 'grab',\n",
       " 'werdenberg',\n",
       " 'cupspiel',\n",
       " 'herisau',\n",
       " 'flames',\n",
       " '1:0',\n",
       " '5:4)sportzentrum',\n",
       " 'zuschauer',\n",
       " 'sr',\n",
       " 'cereda',\n",
       " 'locatelli',\n",
       " 'tor',\n",
       " '15.',\n",
       " 's.',\n",
       " 'meier',\n",
       " 'penalty',\n",
       " '1:0.',\n",
       " '22.',\n",
       " 's.',\n",
       " 'meier',\n",
       " 's.',\n",
       " 'stucki',\n",
       " '2:0.',\n",
       " '25.',\n",
       " 'mattsson',\n",
       " 'bernet',\n",
       " '2:1.',\n",
       " '37.',\n",
       " 'liechti',\n",
       " 'jenny',\n",
       " '42.',\n",
       " 'conzett',\n",
       " 's.',\n",
       " 'meier',\n",
       " '3:2.',\n",
       " '44.',\n",
       " 'conzett',\n",
       " 's.',\n",
       " 'meier',\n",
       " '4:2.',\n",
       " 'germann',\n",
       " 'brand',\n",
       " '5:2.',\n",
       " 'swoboda',\n",
       " '5:3.',\n",
       " 's.',\n",
       " 'meier',\n",
       " 's.',\n",
       " 'stucki',\n",
       " '6:3.',\n",
       " '55.',\n",
       " '54:10',\n",
       " 'b.',\n",
       " 'jud',\n",
       " 'mattsson',\n",
       " 'ausschluss',\n",
       " 'schilling',\n",
       " '6:4.',\n",
       " '54:39',\n",
       " 'dürr',\n",
       " 'j.',\n",
       " 'jud',\n",
       " '6:5.',\n",
       " '56.',\n",
       " '55:40',\n",
       " 'mattsson',\n",
       " 'rautio',\n",
       " '6:6.',\n",
       " '57.',\n",
       " '56:44',\n",
       " 'hess',\n",
       " '7:6.',\n",
       " 'herisau',\n",
       " 'd.',\n",
       " 'jud',\n",
       " 'brunner',\n",
       " 'schwarz',\n",
       " 'rüegg',\n",
       " 'schmid',\n",
       " 'stern',\n",
       " 'l.',\n",
       " 'stucki',\n",
       " 'schweizer',\n",
       " 'hess',\n",
       " 'schilling',\n",
       " 'sandmeier',\n",
       " 's.',\n",
       " 'stucki',\n",
       " 'conzett',\n",
       " 's.',\n",
       " 'meier',\n",
       " 'germann',\n",
       " 'mittelholzer',\n",
       " 'wetter',\n",
       " 'brand',\n",
       " 'strafe',\n",
       " 'je',\n",
       " 'mal',\n",
       " 'minute',\n",
       " 'konzentration',\n",
       " 'schluss',\n",
       " 'herisaus',\n",
       " 'torhüter',\n",
       " 'dominic',\n",
       " 'jud',\n",
       " 'sehen',\n",
       " 'schuss',\n",
       " 'zukommen',\n",
       " 'bild',\n",
       " 'lukas',\n",
       " 'pfiffner']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the tail of the first element of the filtered and lematized data, as there are still quite some unnecessary tokens contained\n",
    "de_tx_lemm[0][200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary variables to save RAM\n",
    "del de_tx_lemm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "4. Supplemental (manual) cleaning of the filtered and lemmatized data\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the filtered and lemmatized data (untokenized)\n",
    "de_tx, de_idx = read_lemmatized('de', tokenize = False) # Overwrite the variable containing the uncleaned data to save RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934308</th>\n",
       "      <td>2441178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934309</th>\n",
       "      <td>2441179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934310</th>\n",
       "      <td>2441180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934311</th>\n",
       "      <td>2441181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934312</th>\n",
       "      <td>2441182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1934313 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          de_idx\n",
       "0          16553\n",
       "1          16554\n",
       "2          16555\n",
       "3          16556\n",
       "4          16557\n",
       "...          ...\n",
       "1934308  2441178\n",
       "1934309  2441179\n",
       "1934310  2441180\n",
       "1934311  2441181\n",
       "1934312  2441182\n",
       "\n",
       "[1934313 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the dataframe containing the according index\n",
    "de_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rückkehrer stefan meier überragen 7:6 flames herisau bangen allerdings schluss lukas pfiffnerin vergangen saison tun uhc herisau darin immer wieder rückstand aufholen partie kehren noch jung 1. liga meisterschaft leben team mindestens heimspiel neu trend deutlich führung noch zittern samstag lagen überzeugend ausserrhoder vorne reagieren ausgleich flames tor dreier minute besassen stefan meier saison wasa verteidigen sommer nla stammverein zurückkehren herausragenden stürmer minute sirene hiess fünft mal meier stock spiel sicherheit ball gehen allerdings auch führung verlieren komplette zusammenbruch drohen flames können aber gewicht nicht total verschieben eindrücklich effort niklas hess tragen gastgeber sieg 57. trainer sagen grosses kino schon woche zuvor meier einzig herisauer niederlage pfannenstiel egg treffer erzielen liegen tor assists nun platz skorer gruppe 2. meier bewegen messen cm grösse kg gewicht erstaunlich geschmeidig können ball behaupten weisen wuchtig schuss so laufen aktuell vorne natürlich schön meinen jährig harmonieren linienpartnern joel conzett silas stucki vorzüglich kreativ trainer nico raschle sprechen grossem kino meier auftritt samstag allgemein einstellung betreffen nicht 1. liga kommen karriere einfach wenig ausplämperlen lassen bestätigen meier indirekt ganz gut neue position zuteilen bekommen da können nochmals richtig reinhängen warum spielen meier herisau vorne erhoffen heute gelingen sagen raschle resultatmässigen notstand muss auch einmal entscheid rückgängig machen flames innert sekunde tor schießen trainer nehmen beordern meier letzt minute verteidigung problem kurzfristig umstellung nicht meinen abwehr machen routine positionsspiel zudem jahrelang torhüter dominic jud rücken verteidigen kennen anweisung genau sonntagabend cupeinsatzfür herisau bringen samstag viert spiel dritt erfolg belegen zweit platz punkt partie bassersdorf nürensdorf saison nicht mehr effektiv punkt erstellung tabelle berücksichtigen quotient bisher geben gruppe corona allerdings noch kein spielabsagen sonntagabend treten ausserrhoder 2. ligisten grab werdenberg cupspiel herisau flames 1:0 5:4)sportzentrum zuschauer sr cereda locatelli tor 15. s. meier penalty 1:0. 22. s. meier s. stucki 2:0. 25. mattsson bernet 2:1. 37. liechti jenny 42. conzett s. meier 3:2. 44. conzett s. meier 4:2. germann brand 5:2. swoboda 5:3. s. meier s. stucki 6:3. 55. 54:10 b. jud mattsson ausschluss schilling 6:4. 54:39 dürr j. jud 6:5. 56. 55:40 mattsson rautio 6:6. 57. 56:44 hess 7:6. herisau d. jud brunner schwarz rüegg schmid stern l. stucki schweizer hess schilling sandmeier s. stucki conzett s. meier germann mittelholzer wetter brand strafe je mal minute konzentration schluss herisaus torhüter dominic jud sehen schuss zukommen bild lukas pfiffner'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first element of the (untokenized) filtered and lemmatized data\n",
    "de_tx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the supplemental/manual cleaning to the filtered and lemmatized data\n",
    "def supp_clean(articles):\n",
    "    # Raise an error if an inappropriate data type is given as an input\n",
    "    if(not isinstance(articles, list)):\n",
    "        raise ValueError(\"Invalid input type. Expected a list.\")\n",
    "\n",
    "    # Keep track of the processing time\n",
    "    t = time.time()\n",
    "    # Remove any instances where 1 to 3 initiating letters are followed by a dot (either once at the end or after each letter), since such cases usually represent abbreviations with low semantic meaning\n",
    "    articles = [re.compile(' [a-z][\\.]?[a-z]?[\\.]?[a-z]?\\.+').sub(' ', x) for x in articles]\n",
    "    # Remove any remaining digit\n",
    "    articles = [re.sub(r'\\b\\d+\\b', '', x) for x in articles]\n",
    "    # Remove anything except words, spaces and the & sign, since this might appear in certain names\n",
    "    articles = [re.sub(r'[^\\w\\s\\&]','', x) for x in articles]\n",
    "    # Remove a list of specific words which appear quite often but do not have any semantic meaning\n",
    "    words_to_remove = ['awp','afp']\n",
    "    for word in words_to_remove:\n",
    "        articles = [re.sub(' '+word, '', x) for x in articles] \n",
    "    # Replace control characters (e.g. \\n or \\t) and multiple blanks with a single blank\n",
    "    articles = [re.sub('\\s+', ' ', x) for x in articles]\n",
    "    # Print out the processing time\n",
    "    print(\"Processing time for supplemental manual cleaning: \", str((time.time() - t)/60), \"minutes\")\n",
    "\n",
    "    # Return the manually cleaned text data\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for supplemental manual cleaning:  7.325034916400909 minutes\n"
     ]
    }
   ],
   "source": [
    "# Apply the supplemental/manual cleaning by means of the previously defined function\n",
    "de_tx = supp_clean(de_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rückkehrer stefan meier überragen flames herisau bangen allerdings schluss lukas pfiffnerin vergangen saison tun uhc herisau darin immer wieder rückstand aufholen partie kehren noch jung liga meisterschaft leben team mindestens heimspiel neu trend deutlich führung noch zittern samstag lagen überzeugend ausserrhoder vorne reagieren ausgleich flames tor dreier minute besassen stefan meier saison wasa verteidigen sommer nla stammverein zurückkehren herausragenden stürmer minute sirene hiess fünft mal meier stock spiel sicherheit ball gehen allerdings auch führung verlieren komplette zusammenbruch drohen flames können aber gewicht nicht total verschieben eindrücklich effort niklas hess tragen gastgeber sieg trainer sagen grosses kino schon woche zuvor meier einzig herisauer niederlage pfannenstiel egg treffer erzielen liegen tor assists nun platz skorer gruppe meier bewegen messen cm grösse kg gewicht erstaunlich geschmeidig können ball behaupten weisen wuchtig schuss so laufen aktuell vorne natürlich schön meinen jährig harmonieren linienpartnern joel conzett silas stucki vorzüglich kreativ trainer nico raschle sprechen grossem kino meier auftritt samstag allgemein einstellung betreffen nicht liga kommen karriere einfach wenig ausplämperlen lassen bestätigen meier indirekt ganz gut neue position zuteilen bekommen da können nochmals richtig reinhängen warum spielen meier herisau vorne erhoffen heute gelingen sagen raschle resultatmässigen notstand muss auch einmal entscheid rückgängig machen flames innert sekunde tor schießen trainer nehmen beordern meier letzt minute verteidigung problem kurzfristig umstellung nicht meinen abwehr machen routine positionsspiel zudem jahrelang torhüter dominic jud rücken verteidigen kennen anweisung genau sonntagabend cupeinsatzfür herisau bringen samstag viert spiel dritt erfolg belegen zweit platz punkt partie bassersdorf nürensdorf saison nicht mehr effektiv punkt erstellung tabelle berücksichtigen quotient bisher geben gruppe corona allerdings noch kein spielabsagen sonntagabend treten ausserrhoder ligisten grab werdenberg cupspiel herisau flames sportzentrum zuschauer sr cereda locatelli tor meier penalty meier stucki mattsson bernet liechti jenny conzett meier conzett meier germann brand swoboda meier stucki jud mattsson ausschluss schilling dürr jud mattsson rautio hess herisau jud brunner schwarz rüegg schmid stern stucki schweizer hess schilling sandmeier stucki conzett meier germann mittelholzer wetter brand strafe je mal minute konzentration schluss herisaus torhüter dominic jud sehen schuss zukommen bild lukas pfiffner'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first element of the fully preprocessed data\n",
    "de_tx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compart the fully preprocessed data to the initial text (copy paste from above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Rückkehrer Stefan Meier überragt beim 7:6 gegen die Flames. Herisau bangt allerdings am Schluss.Lukas PfiffnerIn der vergangenen Saison tat sich der UHC Herisau darin hervor, immer wieder einen Rückstand aufzuholen und Partien zu kehren. In der noch jungen 1.-Liga-Meisterschaft 2020/21 lebt das Team mindestens in den Heimspielen einem neuen Trend nach: trotz deutlicher Führung noch zu zittern.Am Samstag lagen die überzeugenden Ausserrhoder 2:0 vorne, sie reagierten auf den Ausgleich der Flames mit drei Toren innert dreier Minuten. Sie besassen mit Stefan Meier, der während neun Saisons für Wasa verteidigt hat und im Sommer aus der NLA zu seinem Stammverein zurückgekehrt ist, einen herausragenden Stürmer. Elf Minuten vor der Sirene hiess es 6:3, zum fünften Mal hatte Meier seinen Stock im Spiel. Mit der Sicherheit am Ball ging allerdings auch die Führung verloren. Der komplette Zusammenbruch drohte. Die Flames konnten aber die Gewichte nicht total verschieben – und ein eindrücklicher Effort von Niklas Hess trug den Gastgebern den Sieg ein (57.).Der Trainer sagte: «Grosses Kino»Schon eine Woche zuvor hatte Meier bei der einzigen Herisauer Niederlage (5:7 gegen Pfannenstiel Egg) drei Treffer erzielt. Er liegt mit sechs Toren und vier Assists nun auf Platz fünf der Skorer in der Gruppe 2. Meier bewegt sich – gemessen an 190\\xa0cm Grösse und 82 kg Gewicht – erstaunlich geschmeidig, er kann sich am Ball behaupten und weist einen wuchtigen Schuss auf. «Wenn es so läuft wie aktuell, ist es vorne natürlich schön», meinte der 29-Jährige. Er harmonierte mit seinen Linienpartnern Joel Conzett und Silas Stucki vorzüglich. «Beide sind kreativ.» Trainer Nico Raschle sprach von «grossem Kino», was Meiers Auftritt am Samstag und allgemein seine Einstellung betreffe: Er sei nicht in die 1. Liga gekommen, um seine Karriere einfach ein wenig «ausplämperlen» zu lassen. Dies bestätigt Meier indirekt. «Es war ganz gut, eine neue Position zugeteilt zu bekommen.» Da könne man sich nochmals richtig «reinhängen». Warum spielt Meier in Herisau vorne? «Weil wir uns von ihm das erhoffen, was ihm heute gelungen ist», sagte Raschle. Im resultatmässigen Notstand muss man auch einmal einen Entscheid rückgängig machen: Die Flames hatten innert 90 Sekunden drei Tore geschossen, der Trainer nahm ein Time-out und beorderte Meier für die letzten vier Minuten in die Verteidigung zurück. Ein Problem sei die kurzfristige Umstellung nicht gewesen, meinte dieser. «In der Abwehr machst du vieles über die Routine und das Positionsspiel.» Er habe zudem jahrelang mit Torhüter Dominic Jud im Rücken verteidigt. «Ich kenne seine Anweisungen genau.»Am Sonntagabend CupeinsatzFür Herisau brachte der Samstag im vierten Spiel den dritten Erfolg. Es belegt den zweiten Platz (2,25 Punkte pro Partie) hinter Bassersdorf Nürensdorf (2,5). Seit dieser Saison werden nicht mehr die effektiven Punkte für die Erstellung der Tabelle berücksichtigt, sondern die Quotienten. Bisher gab es in der Gruppe 2 trotz Corona allerdings noch keine Spielabsagen. Am Sonntagabend traten die Ausserrhoder beim 2.-Ligisten Grabs-Werdenberg zum Cupspiel an.Herisau – Flames 7:6 (1:0, 1:2, 5:4)Sportzentrum. – 98 Zuschauer. – Sr. Cereda/Locatelli. Tore: 15. S. Meier (Penalty) 1:0. 22. S. Meier (S. Stucki) 2:0. 25. Mattsson (Bernet) 2:1. 37. Liechti (Jenny) 2:2. 42. Conzett (S. Meier) 3:2. 44. (43:47) Conzett (S. Meier) 4:2. 45. (44:31) Germann (Brandes) 5:2. 49. (48:21) Swoboda 5:3. 49. (48:59) S. Meier (S. Stucki) 6:3. 55. (54:10) B. Jud (Mattsson, Ausschluss Schilling) 6:4. 55. (54:39) Dürr (J. Jud) 6:5. 56. (55:40) Mattsson (Rautio) 6:6. 57. (56:44) Hess 7:6.Herisau: D. Jud; Brunner, Schwarz; Rüegg, Schmid; Stern, L. Stucki; Schweizer; Hess, Schilling, Sandmeier; S. Stucki, Conzett, S. Meier; Germann, Mittelholzer, Wetter; Brandes. Strafen: je 1-mal 2 Minuten.Konzentration bis am Schluss: Herisaus Torhüter Dominic Jud sieht einen Schuss auf sich zukommen. Bild: Lukas Pfiffner'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "5. Export the fully preprocessed data as one csv file\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to export the fully preprocessed data\n",
    "def export_preprocessed(language, articles, idx, data_tokenized = True):\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "\n",
    "    # Set the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "\n",
    "    # Untokenize the data if it is still tokenized\n",
    "    if data_tokenized:\n",
    "        # Generate a list containing the fully preprocessed data in form of strings in which all precleaned unigrams are contained and separated by a blank (such that it's easy to read in later)\n",
    "        articles_out = []\n",
    "        for article in articles:\n",
    "            articles_out.append(\" \".join(article))\n",
    "        # Overwrite the variable which stores the tokenized articles\n",
    "        articles = articles_out\n",
    "        # Delete the variable articles_out to save RAM\n",
    "        del articles_out\n",
    "    \n",
    "    # Create a correctly indexed dataframe containing the fully preprocessed data in a column and export it as a csv file\n",
    "    pd.DataFrame(articles, index = idx, columns = ['tx']).to_csv(\"Preprocessed/Sentiment_Analysis/\"+language+\"_preprocessed_senti.csv\", index = True, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the fully preprocessed data\n",
    "export_preprocessed('de', de_tx, de_idx.de_idx.to_list(), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "6. Read in the fully preprocessed data\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read in the fully preprocessed data\n",
    "def read_preprocessed(language, tokenize = True):\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "    \n",
    "    # Set the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "\n",
    "    # Define the name of the file to load\n",
    "    filename = \"Preprocessed/Sentiment_Analysis/\"+language+\"_preprocessed_senti.csv\"\n",
    "\n",
    "    # Read in the dataframe containing the text data\n",
    "    tx_pp = pd.read_csv(filename, index_col = 0, dtype = {'tx': object})\n",
    "\n",
    "    # Get the articles' index together with an enumeration to identify their position in the list of precleaned articles\n",
    "    idx = tx_pp.index\n",
    "    idx = pd.DataFrame(idx, columns = [language+'_idx'])\n",
    "\n",
    "    # Reduce the dataframe to a list containing the text data\n",
    "    tx_pp = tx_pp.tx.to_list()\n",
    "\n",
    "    # Tokenize the data again if tokenize = True (RAM-saving)\n",
    "    if tokenize:\n",
    "        tx_pp = retokenize(tx_pp)\n",
    "\n",
    "    # Return the preprocessed data\n",
    "    return tx_pp, idx\n",
    "\n",
    "# Define a function to retokenize the preprocessed text data (RAM-saving)\n",
    "def retokenize(article_list):\n",
    "    for i in range(len(article_list)):\n",
    "        temp_tx = str(article_list[i]).split()\n",
    "        article_list[i] = temp_tx\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the fully preprocessed data\n",
    "de_tx, de_idx = read_preprocessed('de', tokenize = True) # Overwrite the variables used above to save RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934308</th>\n",
       "      <td>2441178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934309</th>\n",
       "      <td>2441179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934310</th>\n",
       "      <td>2441180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934311</th>\n",
       "      <td>2441181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934312</th>\n",
       "      <td>2441182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1934313 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          de_idx\n",
       "0          16553\n",
       "1          16554\n",
       "2          16555\n",
       "3          16556\n",
       "4          16557\n",
       "...          ...\n",
       "1934308  2441178\n",
       "1934309  2441179\n",
       "1934310  2441180\n",
       "1934311  2441181\n",
       "1934312  2441182\n",
       "\n",
       "[1934313 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the dataframe containing the according index\n",
    "de_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rückkehrer',\n",
       " 'stefan',\n",
       " 'meier',\n",
       " 'überragen',\n",
       " 'flames',\n",
       " 'herisau',\n",
       " 'bangen',\n",
       " 'allerdings',\n",
       " 'schluss',\n",
       " 'lukas',\n",
       " 'pfiffnerin',\n",
       " 'vergangen',\n",
       " 'saison',\n",
       " 'tun',\n",
       " 'uhc',\n",
       " 'herisau',\n",
       " 'darin',\n",
       " 'immer',\n",
       " 'wieder',\n",
       " 'rückstand',\n",
       " 'aufholen',\n",
       " 'partie',\n",
       " 'kehren',\n",
       " 'noch',\n",
       " 'jung',\n",
       " 'liga',\n",
       " 'meisterschaft',\n",
       " 'leben',\n",
       " 'team',\n",
       " 'mindestens',\n",
       " 'heimspiel',\n",
       " 'neu',\n",
       " 'trend',\n",
       " 'deutlich',\n",
       " 'führung',\n",
       " 'noch',\n",
       " 'zittern',\n",
       " 'samstag',\n",
       " 'lagen',\n",
       " 'überzeugend',\n",
       " 'ausserrhoder',\n",
       " 'vorne',\n",
       " 'reagieren',\n",
       " 'ausgleich',\n",
       " 'flames',\n",
       " 'tor',\n",
       " 'dreier',\n",
       " 'minute',\n",
       " 'besassen',\n",
       " 'stefan',\n",
       " 'meier',\n",
       " 'saison',\n",
       " 'wasa',\n",
       " 'verteidigen',\n",
       " 'sommer',\n",
       " 'nla',\n",
       " 'stammverein',\n",
       " 'zurückkehren',\n",
       " 'herausragenden',\n",
       " 'stürmer',\n",
       " 'minute',\n",
       " 'sirene',\n",
       " 'hiess',\n",
       " 'fünft',\n",
       " 'mal',\n",
       " 'meier',\n",
       " 'stock',\n",
       " 'spiel',\n",
       " 'sicherheit',\n",
       " 'ball',\n",
       " 'gehen',\n",
       " 'allerdings',\n",
       " 'auch',\n",
       " 'führung',\n",
       " 'verlieren',\n",
       " 'komplette',\n",
       " 'zusammenbruch',\n",
       " 'drohen',\n",
       " 'flames',\n",
       " 'können',\n",
       " 'aber',\n",
       " 'gewicht',\n",
       " 'nicht',\n",
       " 'total',\n",
       " 'verschieben',\n",
       " 'eindrücklich',\n",
       " 'effort',\n",
       " 'niklas',\n",
       " 'hess',\n",
       " 'tragen',\n",
       " 'gastgeber',\n",
       " 'sieg',\n",
       " 'trainer',\n",
       " 'sagen',\n",
       " 'grosses',\n",
       " 'kino',\n",
       " 'schon',\n",
       " 'woche',\n",
       " 'zuvor',\n",
       " 'meier',\n",
       " 'einzig',\n",
       " 'herisauer',\n",
       " 'niederlage',\n",
       " 'pfannenstiel',\n",
       " 'egg',\n",
       " 'treffer',\n",
       " 'erzielen',\n",
       " 'liegen',\n",
       " 'tor',\n",
       " 'assists',\n",
       " 'nun',\n",
       " 'platz',\n",
       " 'skorer',\n",
       " 'gruppe',\n",
       " 'meier',\n",
       " 'bewegen',\n",
       " 'messen',\n",
       " 'cm',\n",
       " 'grösse',\n",
       " 'kg',\n",
       " 'gewicht',\n",
       " 'erstaunlich',\n",
       " 'geschmeidig',\n",
       " 'können',\n",
       " 'ball',\n",
       " 'behaupten',\n",
       " 'weisen',\n",
       " 'wuchtig',\n",
       " 'schuss',\n",
       " 'so',\n",
       " 'laufen',\n",
       " 'aktuell',\n",
       " 'vorne',\n",
       " 'natürlich',\n",
       " 'schön',\n",
       " 'meinen',\n",
       " 'jährig',\n",
       " 'harmonieren',\n",
       " 'linienpartnern',\n",
       " 'joel',\n",
       " 'conzett',\n",
       " 'silas',\n",
       " 'stucki',\n",
       " 'vorzüglich',\n",
       " 'kreativ',\n",
       " 'trainer',\n",
       " 'nico',\n",
       " 'raschle',\n",
       " 'sprechen',\n",
       " 'grossem',\n",
       " 'kino',\n",
       " 'meier',\n",
       " 'auftritt',\n",
       " 'samstag',\n",
       " 'allgemein',\n",
       " 'einstellung',\n",
       " 'betreffen',\n",
       " 'nicht',\n",
       " 'liga',\n",
       " 'kommen',\n",
       " 'karriere',\n",
       " 'einfach',\n",
       " 'wenig',\n",
       " 'ausplämperlen',\n",
       " 'lassen',\n",
       " 'bestätigen',\n",
       " 'meier',\n",
       " 'indirekt',\n",
       " 'ganz',\n",
       " 'gut',\n",
       " 'neue',\n",
       " 'position',\n",
       " 'zuteilen',\n",
       " 'bekommen',\n",
       " 'da',\n",
       " 'können',\n",
       " 'nochmals',\n",
       " 'richtig',\n",
       " 'reinhängen',\n",
       " 'warum',\n",
       " 'spielen',\n",
       " 'meier',\n",
       " 'herisau',\n",
       " 'vorne',\n",
       " 'erhoffen',\n",
       " 'heute',\n",
       " 'gelingen',\n",
       " 'sagen',\n",
       " 'raschle',\n",
       " 'resultatmässigen',\n",
       " 'notstand',\n",
       " 'muss',\n",
       " 'auch',\n",
       " 'einmal',\n",
       " 'entscheid',\n",
       " 'rückgängig',\n",
       " 'machen',\n",
       " 'flames',\n",
       " 'innert',\n",
       " 'sekunde',\n",
       " 'tor',\n",
       " 'schießen',\n",
       " 'trainer',\n",
       " 'nehmen',\n",
       " 'beordern',\n",
       " 'meier',\n",
       " 'letzt',\n",
       " 'minute',\n",
       " 'verteidigung',\n",
       " 'problem',\n",
       " 'kurzfristig',\n",
       " 'umstellung',\n",
       " 'nicht',\n",
       " 'meinen',\n",
       " 'abwehr',\n",
       " 'machen',\n",
       " 'routine',\n",
       " 'positionsspiel',\n",
       " 'zudem',\n",
       " 'jahrelang',\n",
       " 'torhüter',\n",
       " 'dominic',\n",
       " 'jud',\n",
       " 'rücken',\n",
       " 'verteidigen',\n",
       " 'kennen',\n",
       " 'anweisung',\n",
       " 'genau',\n",
       " 'sonntagabend',\n",
       " 'cupeinsatzfür',\n",
       " 'herisau',\n",
       " 'bringen',\n",
       " 'samstag',\n",
       " 'viert',\n",
       " 'spiel',\n",
       " 'dritt',\n",
       " 'erfolg',\n",
       " 'belegen',\n",
       " 'zweit',\n",
       " 'platz',\n",
       " 'punkt',\n",
       " 'partie',\n",
       " 'bassersdorf',\n",
       " 'nürensdorf',\n",
       " 'saison',\n",
       " 'nicht',\n",
       " 'mehr',\n",
       " 'effektiv',\n",
       " 'punkt',\n",
       " 'erstellung',\n",
       " 'tabelle',\n",
       " 'berücksichtigen',\n",
       " 'quotient',\n",
       " 'bisher',\n",
       " 'geben',\n",
       " 'gruppe',\n",
       " 'corona',\n",
       " 'allerdings',\n",
       " 'noch',\n",
       " 'kein',\n",
       " 'spielabsagen',\n",
       " 'sonntagabend',\n",
       " 'treten',\n",
       " 'ausserrhoder',\n",
       " 'ligisten',\n",
       " 'grab',\n",
       " 'werdenberg',\n",
       " 'cupspiel',\n",
       " 'herisau',\n",
       " 'flames',\n",
       " 'sportzentrum',\n",
       " 'zuschauer',\n",
       " 'sr',\n",
       " 'cereda',\n",
       " 'locatelli',\n",
       " 'tor',\n",
       " 'meier',\n",
       " 'penalty',\n",
       " 'meier',\n",
       " 'stucki',\n",
       " 'mattsson',\n",
       " 'bernet',\n",
       " 'liechti',\n",
       " 'jenny',\n",
       " 'conzett',\n",
       " 'meier',\n",
       " 'conzett',\n",
       " 'meier',\n",
       " 'germann',\n",
       " 'brand',\n",
       " 'swoboda',\n",
       " 'meier',\n",
       " 'stucki',\n",
       " 'jud',\n",
       " 'mattsson',\n",
       " 'ausschluss',\n",
       " 'schilling',\n",
       " 'dürr',\n",
       " 'jud',\n",
       " 'mattsson',\n",
       " 'rautio',\n",
       " 'hess',\n",
       " 'herisau',\n",
       " 'jud',\n",
       " 'brunner',\n",
       " 'schwarz',\n",
       " 'rüegg',\n",
       " 'schmid',\n",
       " 'stern',\n",
       " 'stucki',\n",
       " 'schweizer',\n",
       " 'hess',\n",
       " 'schilling',\n",
       " 'sandmeier',\n",
       " 'stucki',\n",
       " 'conzett',\n",
       " 'meier',\n",
       " 'germann',\n",
       " 'mittelholzer',\n",
       " 'wetter',\n",
       " 'brand',\n",
       " 'strafe',\n",
       " 'je',\n",
       " 'mal',\n",
       " 'minute',\n",
       " 'konzentration',\n",
       " 'schluss',\n",
       " 'herisaus',\n",
       " 'torhüter',\n",
       " 'dominic',\n",
       " 'jud',\n",
       " 'sehen',\n",
       " 'schuss',\n",
       " 'zukommen',\n",
       " 'bild',\n",
       " 'lukas',\n",
       " 'pfiffner']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first element of the fully preprocessed and tokenized data\n",
    "de_tx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "7. Quantitative summary of the data cleaning\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words contained after data cleaning: 488369046\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of words contained after the data cleaning\n",
    "nwords_after = 0\n",
    "for article in de_tx:\n",
    "    nwords_after = nwords_after + len(article)\n",
    "print('Total number of words contained after data cleaning:', nwords_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words per article after data cleaning: 252.47674290562077\n"
     ]
    }
   ],
   "source": [
    "# Get the average number of words per article after the data cleaning\n",
    "avg_nwords_after = nwords_after/len(de_tx)\n",
    "avg_nwords_after\n",
    "print('Average number of words per article after data cleaning:', avg_nwords_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary variables to save RAM\n",
    "del de_tx, de_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the uncleaned data\n",
    "os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "de_tx_uncleaned = pd.read_csv(\"agg_csv_sparse_de.csv\", index_col = 0, dtype = {'so': object, 'la': object, 'tx': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for pre-cleaning:  12.829872250556946 minutes\n",
      "Total number of words contained before data cleaning: 857640778\n"
     ]
    }
   ],
   "source": [
    "## Count the total number of words contained before the data cleaning\n",
    "# Note: to get an appropriate count of the distinct words we must at least apply the sparse preprocessing first, to ensure that all words are separated properly and distracting signs are removed\n",
    "de_tx_uncleaned = pre_clean(de_tx_uncleaned.tx.tolist())\n",
    "# Count the total number of words\n",
    "nwords_before = 0\n",
    "for article in de_tx_uncleaned:\n",
    "    nwords_before = nwords_before + len(article.split())\n",
    "print('Total number of words contained before data cleaning:', nwords_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words per article before data cleaning: 443.38262628643866\n"
     ]
    }
   ],
   "source": [
    "# Get the average number of words per article before the data cleaning\n",
    "avg_nwords_before = nwords_before/len(de_tx_uncleaned)\n",
    "print('Average number of words per article before data cleaning:', avg_nwords_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words removed by the data cleaning: 369271732\n"
     ]
    }
   ],
   "source": [
    "# Get the number of removed words\n",
    "nwords_rm = nwords_before - nwords_after\n",
    "print('Number of words removed by the data cleaning:', nwords_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.0567 percent of the words have been removed by the data cleaning.\n"
     ]
    }
   ],
   "source": [
    "# Get the ratio of the words that have been removed\n",
    "ratio_removed = nwords_rm / nwords_before\n",
    "print(np.round(ratio_removed*100,4),'percent of the words have been removed by the data cleaning.')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89a54e7346d33656f2940aba0e47a561becf96fe36c69c035d9bc66d085d8900"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('Master_Thesis_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
