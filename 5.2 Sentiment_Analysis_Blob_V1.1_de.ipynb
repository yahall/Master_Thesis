{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd089a54e7346d33656f2940aba0e47a561becf96fe36c69c035d9bc66d085d8900",
   "display_name": "Python 3.7.9 64-bit ('Master_Thesis_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h1 style='margin:10px 5px'> \n",
    "Master Thesis Yannik Haller - Sentiment Analysis TEXTBLOB\n",
    "</h1>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "1. Load required packages and the data\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Hallk\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\pylab\\config.py:70: DeprecationWarning: InlineBackend._figure_formats_changed is deprecated in traitlets 4.1: use @observe and @unobserve instead.\n  def _figure_formats_changed(self, name, old, new):\n"
     ]
    }
   ],
   "source": [
    "# Import required baseline packages\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Change pandas' setting to print out long strings\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Spacy (for lemmatization)\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# TextBlob (for Sentiment Analysis)\n",
    "from textblob import Blobber\n",
    "from textblob_de import PatternTagger, PatternAnalyzer\n",
    "\n",
    "# Enable logging for gensim (optional)\n",
    "import logging\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the appropriate working directory\n",
    "os.chdir('D:\\\\Dropbox\\\\MA_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read in the fully preprocessed data\n",
    "def read_preprocessed(language, tokenize = True):\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "    \n",
    "    # Set the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "\n",
    "    # Define the name of the file to load\n",
    "    filename = \"Preprocessed/Sentiment_Analysis/\"+language+\"_preprocessed_senti.csv\"\n",
    "\n",
    "    # Read in the dataframe containing the text data\n",
    "    tx_pp = pd.read_csv(filename, index_col = 0, dtype = {'tx': object})\n",
    "\n",
    "    # Get the articles' index together with an enumeration to identify their position in the list of precleaned articles\n",
    "    idx = tx_pp.index\n",
    "    idx = pd.DataFrame(idx, columns = [language+'_idx'])\n",
    "\n",
    "    # Reduce the dataframe to a list containing the text data\n",
    "    tx_pp = tx_pp.tx.to_list()\n",
    "\n",
    "    # Tokenize the data again if tokenize = True (RAM-saving)\n",
    "    if tokenize:\n",
    "        tx_pp = retokenize(tx_pp)\n",
    "\n",
    "    # Return the preprocessed data\n",
    "    return tx_pp, idx\n",
    "\n",
    "# Define a function to retokenize the preprocessed text data (RAM-saving)\n",
    "def retokenize(article_list):\n",
    "    for i in range(len(article_list)):\n",
    "        temp_tx = str(article_list[i]).split()\n",
    "        article_list[i] = temp_tx\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15474568"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Read in the preprocessed data (not tokenized)\n",
    "de_tx, de_idx = read_preprocessed('de', tokenize = False)\n",
    "\n",
    "# Take a look at the size of the precleaned data\n",
    "sys.getsizeof(de_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'rückkehrer stefan meier überragen flames herisau bangen allerdings schluss lukas pfiffnerin vergangen saison tun uhc herisau darin immer wieder rückstand aufholen partie kehren noch jung liga meisterschaft leben team mindestens heimspiel neu trend deutlich führung noch zittern samstag lagen überzeugend ausserrhoder vorne reagieren ausgleich flames tor dreier minute besassen stefan meier saison wasa verteidigen sommer nla stammverein zurückkehren herausragenden stürmer minute sirene hiess fünft mal meier stock spiel sicherheit ball gehen allerdings auch führung verlieren komplette zusammenbruch drohen flames können aber gewicht nicht total verschieben eindrücklich effort niklas hess tragen gastgeber sieg trainer sagen grosses kino schon woche zuvor meier einzig herisauer niederlage pfannenstiel egg treffer erzielen liegen tor assists nun platz skorer gruppe meier bewegen messen cm grösse kg gewicht erstaunlich geschmeidig können ball behaupten weisen wuchtig schuss so laufen aktuell vorne natürlich schön meinen jährig harmonieren linienpartnern joel conzett silas stucki vorzüglich kreativ trainer nico raschle sprechen grossem kino meier auftritt samstag allgemein einstellung betreffen nicht liga kommen karriere einfach wenig ausplämperlen lassen bestätigen meier indirekt ganz gut neue position zuteilen bekommen da können nochmals richtig reinhängen warum spielen meier herisau vorne erhoffen heute gelingen sagen raschle resultatmässigen notstand muss auch einmal entscheid rückgängig machen flames innert sekunde tor schießen trainer nehmen beordern meier letzt minute verteidigung problem kurzfristig umstellung nicht meinen abwehr machen routine positionsspiel zudem jahrelang torhüter dominic jud rücken verteidigen kennen anweisung genau sonntagabend cupeinsatzfür herisau bringen samstag viert spiel dritt erfolg belegen zweit platz punkt partie bassersdorf nürensdorf saison nicht mehr effektiv punkt erstellung tabelle berücksichtigen quotient bisher geben gruppe corona allerdings noch kein spielabsagen sonntagabend treten ausserrhoder ligisten grab werdenberg cupspiel herisau flames sportzentrum zuschauer sr cereda locatelli tor meier penalty meier stucki mattsson bernet liechti jenny conzett meier conzett meier germann brand swoboda meier stucki jud mattsson ausschluss schilling dürr jud mattsson rautio hess herisau jud brunner schwarz rüegg schmid stern stucki schweizer hess schilling sandmeier stucki conzett meier germann mittelholzer wetter brand strafe je mal minute konzentration schluss herisaus torhüter dominic jud sehen schuss zukommen bild lukas pfiffner'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Take a look at the preprocessed data\n",
    "de_tx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          de_idx\n",
       "1934310  2441180\n",
       "1934311  2441181\n",
       "1934312  2441182"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>de_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1934310</th>\n      <td>2441180</td>\n    </tr>\n    <tr>\n      <th>1934311</th>\n      <td>2441181</td>\n    </tr>\n    <tr>\n      <th>1934312</th>\n      <td>2441182</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Take a look at the dataframe containing the according index\n",
    "de_idx.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1934310, 1934311]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Retrieve the location of the article in the preprocessed data using the according article id\n",
    "article_ids = [2441180, 2441181]\n",
    "location = de_idx[de_idx.de_idx.isin(article_ids)].index.tolist() #1934310\n",
    "\n",
    "# Access the preprocessed text from the articles with the article ids in [2441180, 2441181]\n",
    "#list(de_tx[i] for i in location)\n",
    "\n",
    "# Look at the according location of the articles with the article ids in [2441180, 2441181]\n",
    "location"
   ]
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2. Sentiment Assessment of the Articles\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that evaluates the polarity of the articles and stores the result to a correctly indexed csv file\n",
    "def eval_blob_polarity(tx, idx, outputfile_name = 'de_blob_polarity', batchsize = 100000, first_pos = 0):\n",
    "    # Notes: \n",
    "    ## tx has to be a list containing the precleaned and NOT tokenized articles\n",
    "    ## idx has to be a list containing the correctly ordered index\n",
    "\n",
    "    # Initialize a Blobber class, which uses the language specific PatternAnalyzer we imported above to assess text polarity (sentiment from -1 to 1) and subjectivity\n",
    "    tb = Blobber(pos_tagger = PatternTagger(), analyzer = PatternAnalyzer())\n",
    "\n",
    "    # Set up a loop to go through all articles and evaluate their polarity with TextBlob\n",
    "    i = first_pos\n",
    "    i_last_batch = first_pos\n",
    "    n_articles = len(tx)\n",
    "    pol = []\n",
    "    t = time.time()\n",
    "    for article in tx:\n",
    "        i = i + 1\n",
    "        pol.append(tb(article).sentiment[0])\n",
    "        if i % batchsize == 0 and i != (n_articles-first_pos):\n",
    "            print(\"Processing time to evaluate polarity scores of the articles at positions\", i_last_batch, \"to\", i-1, \":\", str(round((time.time() - t)/60,2)), \"minutes\")\n",
    "            i_last_batch = i\n",
    "            t = time.time()\n",
    "        if i == (n_articles-first_pos):\n",
    "            print(\"Processing time to evaluate polarity scores of the articles at positions\", i_last_batch, \"to\", i-1, \":\", str(round((time.time() - t)/60,2)), \"minutes\")\n",
    "            print(\"DONE! ;)\")\n",
    "\n",
    "    # Create a correctly indexed dataframe\n",
    "    Blob_tx_polarity = pd.DataFrame(pol, index = idx, columns = ['Blob_polarity'])\n",
    "    # Save the results to a csv file\n",
    "    Blob_tx_polarity.to_csv(\"Sentiment/TextBlob/\"+outputfile_name+\".csv\", index = True)\n",
    "    # Return the results\n",
    "    return Blob_tx_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing time to evaluate polarity scores of the articles at positions 0 to 99999: 72.63 minutes\nProcessing time to evaluate polarity scores of the articles at positions 100000 to 199999: 71.35 minutes\nProcessing time to evaluate polarity scores of the articles at positions 200000 to 299999: 73.22 minutes\nProcessing time to evaluate polarity scores of the articles at positions 300000 to 399999: 75.01 minutes\nProcessing time to evaluate polarity scores of the articles at positions 400000 to 499999: 67.63 minutes\nProcessing time to evaluate polarity scores of the articles at positions 500000 to 599999: 84.44 minutes\nProcessing time to evaluate polarity scores of the articles at positions 600000 to 699999: 97.67 minutes\nProcessing time to evaluate polarity scores of the articles at positions 700000 to 799999: 111.3 minutes\nProcessing time to evaluate polarity scores of the articles at positions 800000 to 899999: 97.98 minutes\nProcessing time to evaluate polarity scores of the articles at positions 900000 to 999999: 74.02 minutes\nDONE! ;)\n"
     ]
    }
   ],
   "source": [
    "# Apply the previously defined function on the first half of the German articles\n",
    "Blob_tx_polarity_1 = eval_blob_polarity(de_tx[:1000000], de_idx.de_idx.values.tolist()[:1000000], 'de_blob_polarity_batch1', 100000, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing time to evaluate polarity scores of the articles at positions 1000000 to 1099999 : 73.73 minutes\nProcessing time to evaluate polarity scores of the articles at positions 1100000 to 1199999 : 77.15 minutes\nProcessing time to evaluate polarity scores of the articles at positions 1200000 to 1299999 : 71.26 minutes\nProcessing time to evaluate polarity scores of the articles at positions 1300000 to 1399999 : 75.04 minutes\nProcessing time to evaluate polarity scores of the articles at positions 1400000 to 1499999 : 68.16 minutes\nProcessing time to evaluate polarity scores of the articles at positions 1500000 to 1599999 : 64.96 minutes\nProcessing time to evaluate polarity scores of the articles at positions 1600000 to 1699999 : 60.07 minutes\nProcessing time to evaluate polarity scores of the articles at positions 1700000 to 1799999 : 85.88 minutes\nProcessing time to evaluate polarity scores of the articles at positions 1800000 to 1899999 : 84.49 minutes\nProcessing time to evaluate polarity scores of the articles at positions 1900000 to 1934312 : 28.37 minutes\nDONE! ;)\n"
     ]
    }
   ],
   "source": [
    "# Apply the previously defined function on the second half of the German articles\n",
    "Blob_tx_polarity_2 = eval_blob_polarity(de_tx[1000000:], de_idx.de_idx.values.tolist()[1000000:], 'de_blob_polarity_batch2', 100000, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the results back in, concatenate them to one dataframe and save it as a csv file\n",
    "filenames = [\"Sentiment/TextBlob/de_blob_polarity_batch1.csv\", \"Sentiment/TextBlob/de_blob_polarity_batch2.csv\"]\n",
    "Blob_tx_polarity = pd.concat([pd.read_csv(f, index_col = 0, dtype = {'Blob_polarity': float}) for f in filenames])\n",
    "Blob_tx_polarity.to_csv(\"Sentiment/TextBlob/de_blob_polarity.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Blob_polarity\n",
       "16553         0.310417\n",
       "16554         0.420000\n",
       "16555         0.850000\n",
       "16556         1.000000\n",
       "16557         0.488889\n",
       "...                ...\n",
       "2441178      -0.100000\n",
       "2441179       0.121429\n",
       "2441180       0.350000\n",
       "2441181      -0.433333\n",
       "2441182       0.558929\n",
       "\n",
       "[1934313 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Blob_polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>16553</th>\n      <td>0.310417</td>\n    </tr>\n    <tr>\n      <th>16554</th>\n      <td>0.420000</td>\n    </tr>\n    <tr>\n      <th>16555</th>\n      <td>0.850000</td>\n    </tr>\n    <tr>\n      <th>16556</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>16557</th>\n      <td>0.488889</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2441178</th>\n      <td>-0.100000</td>\n    </tr>\n    <tr>\n      <th>2441179</th>\n      <td>0.121429</td>\n    </tr>\n    <tr>\n      <th>2441180</th>\n      <td>0.350000</td>\n    </tr>\n    <tr>\n      <th>2441181</th>\n      <td>-0.433333</td>\n    </tr>\n    <tr>\n      <th>2441182</th>\n      <td>0.558929</td>\n    </tr>\n  </tbody>\n</table>\n<p>1934313 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Take a look at the results\n",
    "Blob_tx_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the concatenated results back in\n",
    "Blob_tx_polarity = pd.read_csv(\"Sentiment/TextBlob/de_blob_polarity.csv\", index_col = 0, dtype = {'Blob_polarity': float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Blob_polarity\n",
       "16553         0.310417\n",
       "16554         0.420000\n",
       "16555         0.850000\n",
       "16556         1.000000\n",
       "16557         0.488889\n",
       "...                ...\n",
       "2441178      -0.100000\n",
       "2441179       0.121429\n",
       "2441180       0.350000\n",
       "2441181      -0.433333\n",
       "2441182       0.558929\n",
       "\n",
       "[1934313 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Blob_polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>16553</th>\n      <td>0.310417</td>\n    </tr>\n    <tr>\n      <th>16554</th>\n      <td>0.420000</td>\n    </tr>\n    <tr>\n      <th>16555</th>\n      <td>0.850000</td>\n    </tr>\n    <tr>\n      <th>16556</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>16557</th>\n      <td>0.488889</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2441178</th>\n      <td>-0.100000</td>\n    </tr>\n    <tr>\n      <th>2441179</th>\n      <td>0.121429</td>\n    </tr>\n    <tr>\n      <th>2441180</th>\n      <td>0.350000</td>\n    </tr>\n    <tr>\n      <th>2441181</th>\n      <td>-0.433333</td>\n    </tr>\n    <tr>\n      <th>2441182</th>\n      <td>0.558929</td>\n    </tr>\n  </tbody>\n</table>\n<p>1934313 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Take a look at the read in results\n",
    "Blob_tx_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The share of articles with a positive sentiment is 69.0 %\nThe share of articles with a negative sentiment is 27.0 %\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Blob_polarity\n",
       "count    1934313.000\n",
       "mean           0.177\n",
       "std            0.401\n",
       "min           -1.000\n",
       "25%           -0.028\n",
       "50%            0.198\n",
       "75%            0.434\n",
       "max            1.000"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Blob_polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1934313.000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.177</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.401</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.028</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.198</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.434</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Take a look at some summary statistics\n",
    "share_pos = np.round(np.sum(Blob_tx_polarity['Blob_polarity'] > 0) / len(Blob_tx_polarity),2)\n",
    "share_neg = np.round(np.sum(Blob_tx_polarity['Blob_polarity'] < 0) / len(Blob_tx_polarity),2)\n",
    "print('The share of articles with a positive sentiment is', 100*share_pos,'%')\n",
    "print('The share of articles with a negative sentiment is', 100*share_neg,'%')\n",
    "np.round(Blob_tx_polarity.describe(), 3)"
   ]
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "TRYOUTS!!!! & some usefull codes\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_lg', disable = ['tok2vec', 'morphologizer', 'senter', 'ner', 'attribute_ruler', 'lemmatizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sie ich PRON sb True\nsieht sehen VERB ROOT False\nwirklich wirklich ADJ mo True\nsehr sehr ADV mo True\ngut gut ADJ mo True\naus aus ADP svp True\n. . PUNCT punct False\nEr ich PRON sb True\nist sein AUX ROOT True\nenorm enorm ADJ mo False\nstark stark ADJ mo False\nverletzt verletzen VERB pd False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Sie sieht wirklich sehr gut aus. Er ist enorm stark verletzt')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nie nie ADV mo True\nnichts nichts PRON ROOT True\nnicht nicht PART ng True\nkein kein DET mo True\nkeine kein DET nk True\nkeiner kein PRON sb True\nkaum kaum ADV mo True\nwenig wenig PRON mo True\nohne ohne ADP mo True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('nie nichts nicht kein keine keiner kaum wenig ohne')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Es ich PRON sb True\ngefällt gefallen VERB ROOT False\nmir sich PRON da True\nweniger wenig PRON mo True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Es gefällt mir weniger')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob (for Sentiment Analysis)\n",
    "from textblob import Blobber\n",
    "from textblob_de import PatternTagger, PatternAnalyzer\n",
    "# Initialize a Blobber class, which uses the language specific PatternAnalyzer we imported above to assess text polarity (sentiment from -1 to 1) and subjectivity\n",
    "tb = Blobber(pos_tagger = PatternTagger(), analyzer = PatternAnalyzer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.5, subjectivity=0.0)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "tb('ohne sein scooter sind wir nicht gut').sentiment"
   ]
  }
 ]
}