{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h1 style='margin:10px 5px'> \n",
    "Master Thesis Yannik Haller - Data Preprocessing LDA\n",
    "</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "1. Load required packages and the data\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required baseline packages\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Change pandas' setting to print out long strings\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "# Spacy (for lemmatization)\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the appropriate working directory\n",
    "os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "\n",
    "# Read in the aggregated data\n",
    "fr_tx = pd.read_csv(\"agg_csv_sparse_fr.csv\", index_col = 0, dtype = {'so': object, 'la': object, 'tx': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(481162, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the shape of the data\n",
    "fr_tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the article IDs (i.e. index)\n",
    "fr_idx = fr_tx.index  # French"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2. Preprocess the text data batchwise\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "Run these commands in terminal (cmd) after the appropriate environment has been activated (by running the command \"activate Master_Thesis_env\") in order to install the required spaCy implementations.\n",
    "\n",
    "## German\n",
    "python -m spacy download de_core_news_sm (check)\n",
    "\n",
    "python -m spacy download de_core_news_md (check)\n",
    "\n",
    "python -m spacy download de_core_news_lg (check)\n",
    "\n",
    "## English\n",
    "python -m spacy download en_core_web_sm (check)\n",
    "\n",
    "python -m spacy download en_core_web_md (check)\n",
    "\n",
    "python -m spacy download en_core_web_lg (check)\n",
    "\n",
    "## French\n",
    "python -m spacy download fr_core_news_sm (check)\n",
    "\n",
    "python -m spacy download fr_core_news_md (check)\n",
    "\n",
    "python -m spacy download fr_core_news_lg (check)\n",
    "\n",
    "## Italian\n",
    "python -m spacy download it_core_news_sm (check)\n",
    "\n",
    "python -m spacy download it_core_news_md (check)\n",
    "\n",
    "python -m spacy download it_core_news_lg (check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2.1 Define all required functions to preprocess the data (pre-cleaning, tokenizing, removing stop words and lemmatization)\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define all required functions for the batchwise data preprocessing\n",
    "\n",
    "# Define a function to prepare/pre-clean the text data\n",
    "def pre_clean(articles):\n",
    "    # Raise an error if an inappropriate data type is given as an input\n",
    "    if(not isinstance(articles, list)):\n",
    "        raise ValueError(\"Invalid input type. Expected a list.\")\n",
    "\n",
    "    # Keep track of the processing time\n",
    "    t = time.time()\n",
    "\n",
    "    # Remove any links starting with http:// or https://\n",
    "    articles = [re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+').sub('',x) for x in articles]\n",
    "    # Remove any links starting with www.\n",
    "    articles = [re.compile('www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+').sub('',x) for x in articles]\n",
    "    \n",
    "    # Replace punctuations which are not followed by a blank with punctuations followed by a blank\n",
    "    articles = [re.sub(r'[\\.]', '. ', x) for x in articles]\n",
    "    # Separate words in which a lowercase letter is followed by a capital letter, since they usually do not belong together\n",
    "    articles = [re.sub('(^[a-z]*)+([A-Z])', r'\\1 \\2', x) for x in articles]\n",
    "    # Correct manually for those cases where a name like 'McDonalds' was separated to Mc Donalds\n",
    "    articles = [re.sub('Mc ', 'Mc', x) for x in articles]\n",
    "    # Replace quotation marks with a blank\n",
    "    articles = [re.sub('«', ' ', x) for x in articles]\n",
    "    articles = [re.sub('»', ' ', x) for x in articles]\n",
    "    # Remove percentage signs\n",
    "    articles = [re.sub('%', ' ', x) for x in articles]\n",
    "    # Remove distracting hyphens\n",
    "    articles = [re.sub(\"-\", \" \", x) for x in articles]\n",
    "    articles = [re.sub(\"–\", \" \", x) for x in articles]\n",
    "    # Replace control characters (e.g. \\n or \\t) and multiple blanks with a single blank\n",
    "    articles = [re.sub('\\s+', ' ', x) for x in articles]\n",
    "    # Print out the processing time\n",
    "    print(\"Processing time for pre-cleaning: \", str((time.time() - t)/60), \"minutes\")\n",
    "\n",
    "    # Return the pre-cleaned text data\n",
    "    return articles\n",
    "\n",
    "\n",
    "# Define a function to perform the following tasks at once:\n",
    "## 1. Tokenize: transform text into a list of words, digits and punctuations\n",
    "## 2. Remove stopwords\n",
    "## 3. Filter the tokens, such that only nouns, proper nouns, verbs, adjectives and adverbs are kept, while digits and punctuations are removed\n",
    "## 4. Lemmatize: transform each word back to its word stem\n",
    "## 5. Lowercase the entire data\n",
    "def tokenize_filter_and_lemmatize(articles, nlp):\n",
    "    # Keep track of the processing time\n",
    "    t = time.time()\n",
    "    # Create a list to store the output\n",
    "    articles_out = []\n",
    "    # Define the list of allowed postags (pos = part of speech)\n",
    "    allowed_postags = ['PROPN', 'NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    # Create a loop to go through all articles in the input list of articles\n",
    "    for article in articles:\n",
    "        # Define the current article as the focal document\n",
    "        doc = nlp(article)\n",
    "        # Tokenize, filter and lemmatize the document, while filtering out stop words, punctuations and unused word types (i.e. words with a postag that is not contained in the 'allowed_posttags' variable)\n",
    "        articles_out.append([token.lemma_.lower() for token in doc if (token.pos_ in allowed_postags and not token.is_stop)])\n",
    "    # Print out the processing time\n",
    "    print(\"Processing time for tokenizing, filtering and lemmatization: \", str((time.time() - t)/60), \"minutes\")\n",
    "    return articles_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2.2 Define a function to preprocess and export the data batchwise\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the function to apply batchwise processing of the text data\n",
    "\n",
    "# Note: \"articles\" has to be a dataframe with a column tx containing the text files\n",
    "def process_batchwise(articles, language, batch_size = 100000, first_batch_number = 1):\n",
    "    # Raise an error if an inappropriate data type is given as an input\n",
    "    if(not isinstance(articles, pd.DataFrame)):\n",
    "        raise ValueError(\"Invalid input type. Expected a pandas DataFrame.\")\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "\n",
    "    # Get the number of batches\n",
    "    nbatches = int((len(articles)-1)/batch_size) + 1\n",
    "    # Store the index of the articles\n",
    "    idx = articles.index\n",
    "    # Convert the column of the dataframe that contains the articles to a list of articles, while overwriting the variable 'articles' to save RAM\n",
    "    articles = articles.tx.values.tolist()\n",
    "\n",
    "    # Initialize the appropriate spacy model depending on the language of the text data, while keeping only the tagger component (for efficiency)\n",
    "    if language == 'de':\n",
    "        nlp = spacy.load('de_core_news_lg', disable = ['tok2vec', 'morphologizer', 'senter', 'ner', 'attribute_ruler'])\n",
    "    elif language == 'en':\n",
    "        nlp = spacy.load('en_core_web_lg', disable = ['tok2vec', 'morphologizer', 'senter', 'ner', 'attribute_ruler'])\n",
    "    elif language == 'fr':\n",
    "        nlp = spacy.load('fr_core_news_lg', disable = ['tok2vec', 'morphologizer', 'senter', 'ner', 'attribute_ruler'])\n",
    "    elif language == 'it':\n",
    "        nlp = spacy.load('it_core_news_lg', disable = ['tok2vec', 'morphologizer', 'senter', 'ner', 'attribute_ruler'])\n",
    "    \n",
    "    # Set up a loop to process the data batchwise\n",
    "    for i in range(nbatches):\n",
    "        print('Processing batch #', i+first_batch_number, '...')\n",
    "        # Select the data related to the current batch\n",
    "        batch_min = batch_size * i\n",
    "        if i == (nbatches - 1):\n",
    "            batch_max = len(articles)\n",
    "        else:\n",
    "            batch_max = batch_size * (i+1)\n",
    "        batch_tx = articles[batch_min:batch_max]\n",
    "\n",
    "        # Pre-clean the data\n",
    "        batch_tx = pre_clean(batch_tx)\n",
    "        # Tokenize, filter and lemmatize the data\n",
    "        batch_tx = tokenize_filter_and_lemmatize(batch_tx, nlp)\n",
    "\n",
    "        ## Save the processed text data to a csv file\n",
    "        # Generate a list containing the preprocessed data in form of strings in which all lemmatized phrases are contained and separated by a blank (such that it's easy to read in later)\n",
    "        batch_tx_out = []\n",
    "        for article in batch_tx:\n",
    "            batch_tx_out.append(\" \".join(article))\n",
    "        # Create a correctly indexed dataframe containing the preprocessed data in a column and export it as a csv file\n",
    "        pd.DataFrame(batch_tx_out, index = idx[batch_min:batch_max], columns = ['tx']).to_csv(\n",
    "            \"Preprocessed/Lemmatized/\"+language+\"_lemmatized_batch_\"+str(i+first_batch_number)+\".csv\", index = True, encoding = 'utf-8-sig'\n",
    "        )\n",
    "\n",
    "        # Delete large unused variables to save memory\n",
    "        del batch_tx_out\n",
    "    print(\"DONE! ;)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2.3 Apply batchwise preprocessing and store the preprocessed data externally as csv files\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch # 1 ...\n",
      "Processing time for pre-cleaning:  0.4011604468027751 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  76.61191082000732 minutes\n",
      "Processing batch # 2 ...\n",
      "Processing time for pre-cleaning:  0.4837564984957377 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  79.22954199314117 minutes\n",
      "Processing batch # 3 ...\n",
      "Processing time for pre-cleaning:  0.45687848726908364 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  73.37979460557303 minutes\n",
      "Processing batch # 4 ...\n",
      "Processing time for pre-cleaning:  0.5389755050341288 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  105.8040094335874 minutes\n",
      "Processing batch # 5 ...\n",
      "Processing time for pre-cleaning:  0.36118507385253906 minutes\n",
      "Processing time for tokenizing, filtering and lemmatization:  65.33602706988653 minutes\n",
      "DONE! ;)\n"
     ]
    }
   ],
   "source": [
    "# Apply batchwise preprocessing by means of the previously defined function\n",
    "process_batchwise(fr_tx, language = 'fr', batch_size = 100000, first_batch_number = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "3. Read in and inspect the filtered and lemmatized data\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read in and concatenate the filtered and lemmatized data\n",
    "def read_lemmatized(language, tokenize = True):\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "\n",
    "    # Set the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data\\\\Preprocessed\\\\Lemmatized')\n",
    "\n",
    "    # Get a list of all files to read and concatenate\n",
    "    extension = 'csv'\n",
    "    all_filenames = [i for i in glob.glob(language+\"_lemmatized_batch_*.{}\".format(extension))]\n",
    "    # Concatenate all files in the list to one dataframe\n",
    "    batches_aggregated = pd.concat([pd.read_csv(f, index_col = 0, dtype = {'tx': object}) for f in all_filenames])\n",
    "    # Get the articles' indices together with an enumeration to identify them in the list of filtered and lemmatized articles\n",
    "    idx = batches_aggregated.index\n",
    "    idx = pd.DataFrame(idx, columns = [language+'_idx'])\n",
    "    # Tokenize the data again if tokenize = True\n",
    "    if tokenize:\n",
    "        batches_aggregated = retokenize(batches_aggregated.tx.values.tolist())\n",
    "    else:\n",
    "        batches_aggregated = batches_aggregated.tx.values.tolist()\n",
    "    \n",
    "    # Reset the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data')  \n",
    "\n",
    "    # Return the precleaned data\n",
    "    return batches_aggregated, idx\n",
    "\n",
    "# Define a function to retokenize the filtered and lemmatized text data\n",
    "def retokenize(articles):\n",
    "    articles_out = []\n",
    "    for article in articles:\n",
    "        articles_out.append(article.split())\n",
    "    return articles_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the filtered and lemmatized data\n",
    "fr_tx_lemm, fr_idx = read_lemmatized('fr', tokenize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fr_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481157</th>\n",
       "      <td>2436478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481158</th>\n",
       "      <td>2436479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481159</th>\n",
       "      <td>2436480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481160</th>\n",
       "      <td>2436481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481161</th>\n",
       "      <td>2436482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>481162 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fr_idx\n",
       "0             0\n",
       "1             1\n",
       "2             2\n",
       "3             3\n",
       "4             4\n",
       "...         ...\n",
       "481157  2436478\n",
       "481158  2436479\n",
       "481159  2436480\n",
       "481160  2436481\n",
       "481161  2436482\n",
       "\n",
       "[481162 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the dataframe containing the according index\n",
    "fr_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4290016"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the size of the filtered and lemmatized data\n",
    "sys.getsizeof(fr_tx_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bourse', 'york', 'terminer', 'hausse', 'mercredi', 'espoir']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first few tokens of the first element of filtered and lemmatized data\n",
    "fr_tx_lemm[0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"La Bourse de New York a terminé en hausse mercredi, sur les espoirs d'un prochain accord sur un nouveau plan d'aide économique américain qui a mené le Dow Jones brièvement au-dessus de 2% en séance.Le Dow Jones Industrial Average a avancé de 1,20% à 27.781,70 points. Le Nasdaq a gagné 0,74% à 11.167,50 points et le S&P 500, a progressé de 1,05% à 3370,53 points.La Bourse de New York avait clôturé anxieusement en légère baisse mardi avant le débat présidentiel. Le Dow Jones Industrial Average, avait cédé 0,48% et le Nasdaq -0,29%.Mercredi, la rencontre entre la cheffe des démocrates à la Chambre et le secrétaire américain au Trésor pour discuter d'une nouvelle aide économique, en panne depuis des mois, a suscité l'espoir d'un «compromis raisonnable», selon les mots de Steven Mnuchin.Cet optimisme a donné un coup de fouet aux actions, qui s'est brusquement tempéré «lorsque le chef des républicains au Sénat Mitch McConnell est sorti et a dit que les positions étaient encore très, très éloignées», a expliqué Karl Haeling de LBBW.La Bourse de New York a vu aussi l'introduction en fanfare, via une cotation directe, des titres du discret groupe de surveillance de données Palantir, à un prix le valorisant à plus de 21 milliards de dollars. Sous le symbole PLTR, le titre a clôturé à 9,73 dollars, soit bien au dessus du prix indicatif de 7,25 dollars donné mardi soir par le New York Stock Exchange (lire page 11)Le titre du fabricant de camions électriques et à hydrogène Nikola a repris de la vigueur (+14,54% à 20,48 dollars) après sa descente aux enfers marquée par la perte de deux tiers de sa valeur depuis son introduction en bourse.Nikola a ajourné mercredi un événement au cours duquel il devait présenter en grande pompe son nouveau pick-up Badger.Quasiment tous les secteurs du S&P ont terminé dans le vert, celui de la santé en tête. les laboratoires Pfizer ont pris 1,41%. Des grands noms de la tech ont progressé comme Microsoft et Apple (+1,50%).Sur le marché obligataire, le taux à 10 ans sur la dette américaine augmentait à 0,6840% contre 0,6495% mardi soir. – (afp)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare it to the initial text\n",
    "fr_tx.tx.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary variables to save RAM\n",
    "del fr_tx_lemm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "4. Supplemental (manual) cleaning of the filtered and lemmatized data\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the filtered and lemmatized data (untokenized)\n",
    "fr_tx, fr_idx = read_lemmatized('fr', tokenize = False) # Overwrite the variable containing the uncleaned data to save RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fr_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481157</th>\n",
       "      <td>2436478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481158</th>\n",
       "      <td>2436479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481159</th>\n",
       "      <td>2436480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481160</th>\n",
       "      <td>2436481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481161</th>\n",
       "      <td>2436482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>481162 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fr_idx\n",
       "0             0\n",
       "1             1\n",
       "2             2\n",
       "3             3\n",
       "4             4\n",
       "...         ...\n",
       "481157  2436478\n",
       "481158  2436479\n",
       "481159  2436480\n",
       "481160  2436481\n",
       "481161  2436482\n",
       "\n",
       "[481162 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the dataframe containing the according index\n",
    "fr_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bourse york terminer hausse mercredi espoir prochain accord plan aide économique américain mener dow jones brièvement séance dow jones industrial average avancer 781,70 point nasdaq gagner point s&p progresser 1,05 point bourse york clôturer anxieusement léger baisse mardi débat présidentiel dow jones industrial average céder nasdaq mercredi rencontre cheffe démocrate chambre secrétaire américain trésor discuter nouveau aide économique panne mois susciter espoir compromis raisonnable mot steven mnuchin optimisme donner coup fouet action brusquement tempérer chef républicain sénat mitch mcconnell sortir position éloigné expliquer karl haeling lbbw bourse york introduction fanfare cotation titre discret groupe surveillance donnée palantir prix valoriser milliard dollar symbole pltr titre clôturer dollar prix indicatif dollar donner mardi soir new york stock lire page titre fabricant camion électrique hydrogène nikola reprendre vigueur +14,54 dollar descente enfer marquer perte tiers valeur introduction bourse nikola ajourner mercredi événement cours devoir présenter grand pompe pick up badger quasiment secteur s&p terminer vert santé tête laboratoire pfizer prendre grand nom tech progresser microsoft apple +1,50 marché obligataire taux an dette américain augmenter mardi soir afp'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first element of the (untokenized) filtered and lemmatized data\n",
    "fr_tx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the supplemental/manual cleaning to the filtered and lemmatized data\n",
    "def supp_clean(articles):\n",
    "    # Raise an error if an inappropriate data type is given as an input\n",
    "    if(not isinstance(articles, list)):\n",
    "        raise ValueError(\"Invalid input type. Expected a list.\")\n",
    "\n",
    "    # Keep track of the processing time\n",
    "    t = time.time()\n",
    "    # Remove any instances where 1 to 3 initiating letters are followed by a dot (either once at the end or after each letter), since such cases usually represent abbreviations with low semantic meaning\n",
    "    articles = [re.compile(' [a-z][\\.]?[a-z]?[\\.]?[a-z]?\\.+').sub(' ', x) for x in articles]\n",
    "    # Remove any remaining digit\n",
    "    articles = [re.sub(r'\\b\\d+\\b', '', x) for x in articles]\n",
    "    # Remove anything except words, spaces and the & sign, since this might appear in certain names\n",
    "    articles = [re.sub(r'[^\\w\\s\\&]','', x) for x in articles]\n",
    "    # Remove a list of specific words which appear quite often but do not seem to add any semantic value\n",
    "    words_to_remove = ['awp','afp']\n",
    "    for word in words_to_remove:\n",
    "        articles = [re.sub(' '+word, '', x) for x in articles]   \n",
    "    # Replace control characters (e.g. \\n or \\t) and multiple blanks with a single blank\n",
    "    articles = [re.sub('\\s+', ' ', x) for x in articles]\n",
    "    # Print out the processing time\n",
    "    print(\"Processing time for supplemental manual cleaning: \", str((time.time() - t)/60), \"minutes\")\n",
    "\n",
    "    # Return the manually cleaned text data\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for supplemental manual cleaning:  1.3890565633773804 minutes\n"
     ]
    }
   ],
   "source": [
    "# Apply the supplemental/manual cleaning by means of the previously defined function\n",
    "fr_tx = supp_clean(fr_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bourse york terminer hausse mercredi espoir prochain accord plan aide économique américain mener dow jones brièvement séance dow jones industrial average avancer point nasdaq gagner point s&p progresser point bourse york clôturer anxieusement léger baisse mardi débat présidentiel dow jones industrial average céder nasdaq mercredi rencontre cheffe démocrate chambre secrétaire américain trésor discuter nouveau aide économique panne mois susciter espoir compromis raisonnable mot steven mnuchin optimisme donner coup fouet action brusquement tempérer chef républicain sénat mitch mcconnell sortir position éloigné expliquer karl haeling lbbw bourse york introduction fanfare cotation titre discret groupe surveillance donnée palantir prix valoriser milliard dollar symbole pltr titre clôturer dollar prix indicatif dollar donner mardi soir new york stock lire page titre fabricant camion électrique hydrogène nikola reprendre vigueur dollar descente enfer marquer perte tiers valeur introduction bourse nikola ajourner mercredi événement cours devoir présenter grand pompe pick up badger quasiment secteur s&p terminer vert santé tête laboratoire pfizer prendre grand nom tech progresser microsoft apple marché obligataire taux an dette américain augmenter mardi soir'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first element of the fully preprocessed data\n",
    "fr_tx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the fully preprocessed data to the initial text (copy paste from above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"La Bourse de New York a terminé en hausse mercredi, sur les espoirs d'un prochain accord sur un nouveau plan d'aide économique américain qui a mené le Dow Jones brièvement au-dessus de 2% en séance.Le Dow Jones Industrial Average a avancé de 1,20% à 27.781,70 points. Le Nasdaq a gagné 0,74% à 11.167,50 points et le S&P 500, a progressé de 1,05% à 3370,53 points.La Bourse de New York avait clôturé anxieusement en légère baisse mardi avant le débat présidentiel. Le Dow Jones Industrial Average, avait cédé 0,48% et le Nasdaq -0,29%.Mercredi, la rencontre entre la cheffe des démocrates à la Chambre et le secrétaire américain au Trésor pour discuter d'une nouvelle aide économique, en panne depuis des mois, a suscité l'espoir d'un «compromis raisonnable», selon les mots de Steven Mnuchin.Cet optimisme a donné un coup de fouet aux actions, qui s'est brusquement tempéré «lorsque le chef des républicains au Sénat Mitch McConnell est sorti et a dit que les positions étaient encore très, très éloignées», a expliqué Karl Haeling de LBBW.La Bourse de New York a vu aussi l'introduction en fanfare, via une cotation directe, des titres du discret groupe de surveillance de données Palantir, à un prix le valorisant à plus de 21 milliards de dollars. Sous le symbole PLTR, le titre a clôturé à 9,73 dollars, soit bien au dessus du prix indicatif de 7,25 dollars donné mardi soir par le New York Stock Exchange (lire page 11)Le titre du fabricant de camions électriques et à hydrogène Nikola a repris de la vigueur (+14,54% à 20,48 dollars) après sa descente aux enfers marquée par la perte de deux tiers de sa valeur depuis son introduction en bourse.Nikola a ajourné mercredi un événement au cours duquel il devait présenter en grande pompe son nouveau pick-up Badger.Quasiment tous les secteurs du S&P ont terminé dans le vert, celui de la santé en tête. les laboratoires Pfizer ont pris 1,41%. Des grands noms de la tech ont progressé comme Microsoft et Apple (+1,50%).Sur le marché obligataire, le taux à 10 ans sur la dette américaine augmentait à 0,6840% contre 0,6495% mardi soir. – (afp)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "5. Export the fully preprocessed data as one csv file\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to export the fully preprocessed data\n",
    "def export_preprocessed(language, articles, idx, data_tokenized = True):\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "\n",
    "    # Set the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "\n",
    "    # Untokenize the data if it is still tokenized\n",
    "    if data_tokenized:\n",
    "        # Generate a list containing the fully preprocessed data in form of strings in which all precleaned unigrams are contained and separated by a blank (such that it's easy to read in later)\n",
    "        articles_out = []\n",
    "        for article in articles:\n",
    "            articles_out.append(\" \".join(article))\n",
    "        # Overwrite the variable which stores the tokenized articles\n",
    "        articles = articles_out\n",
    "        # Delete the variable articles_out to save RAM\n",
    "        del articles_out\n",
    "    \n",
    "    # Create a correctly indexed dataframe containing the fully preprocessed data in a column and export it as a csv file\n",
    "    pd.DataFrame(articles, index = idx, columns = ['tx']).to_csv(\"Preprocessed/\"+language+\"_preprocessed.csv\", index = True, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the fully preprocessed data\n",
    "export_preprocessed('fr', fr_tx, fr_idx.fr_idx.to_list(), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "6. Read in the fully preprocessed data\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read in the fully preprocessed data\n",
    "def read_preprocessed(language, tokenize = True):\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "    \n",
    "    # Set the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "\n",
    "    # Define the name of the file to load\n",
    "    filename = \"Preprocessed/\"+language+\"_preprocessed.csv\"\n",
    "\n",
    "    # Read in the dataframe containing the text data\n",
    "    tx_pp = pd.read_csv(filename, index_col = 0, dtype = {'tx': object})\n",
    "\n",
    "    # Get the articles' index together with an enumeration to identify their position in the list of precleaned articles\n",
    "    idx = tx_pp.index\n",
    "    idx = pd.DataFrame(idx, columns = [language+'_idx'])\n",
    "\n",
    "    # Reduce the dataframe to a list containing the text data\n",
    "    tx_pp = tx_pp.tx.to_list()\n",
    "\n",
    "    # Tokenize the data again if tokenize = True (RAM-saving)\n",
    "    if tokenize:\n",
    "        tx_pp = retokenize(tx_pp)\n",
    "\n",
    "    # Return the preprocessed data\n",
    "    return tx_pp, idx\n",
    "\n",
    "# Define a function to retokenize the preprocessed text data (RAM-saving)\n",
    "def retokenize(article_list):\n",
    "    for i in range(len(article_list)):\n",
    "        temp_tx = str(article_list[i]).split()\n",
    "        article_list[i] = temp_tx\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the fully preprocessed data\n",
    "fr_tx, fr_idx = read_preprocessed('fr', tokenize = True) # Overwrite the variables used above to save RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fr_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481157</th>\n",
       "      <td>2436478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481158</th>\n",
       "      <td>2436479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481159</th>\n",
       "      <td>2436480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481160</th>\n",
       "      <td>2436481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481161</th>\n",
       "      <td>2436482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>481162 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fr_idx\n",
       "0             0\n",
       "1             1\n",
       "2             2\n",
       "3             3\n",
       "4             4\n",
       "...         ...\n",
       "481157  2436478\n",
       "481158  2436479\n",
       "481159  2436480\n",
       "481160  2436481\n",
       "481161  2436482\n",
       "\n",
       "[481162 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the dataframe containing the according index\n",
    "fr_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bourse',\n",
       " 'york',\n",
       " 'terminer',\n",
       " 'hausse',\n",
       " 'mercredi',\n",
       " 'espoir',\n",
       " 'prochain',\n",
       " 'accord',\n",
       " 'plan',\n",
       " 'aide',\n",
       " 'économique',\n",
       " 'américain',\n",
       " 'mener',\n",
       " 'dow',\n",
       " 'jones',\n",
       " 'brièvement',\n",
       " 'séance',\n",
       " 'dow',\n",
       " 'jones',\n",
       " 'industrial',\n",
       " 'average',\n",
       " 'avancer',\n",
       " 'point',\n",
       " 'nasdaq',\n",
       " 'gagner',\n",
       " 'point',\n",
       " 's&p',\n",
       " 'progresser',\n",
       " 'point',\n",
       " 'bourse',\n",
       " 'york',\n",
       " 'clôturer',\n",
       " 'anxieusement',\n",
       " 'léger',\n",
       " 'baisse',\n",
       " 'mardi',\n",
       " 'débat',\n",
       " 'présidentiel',\n",
       " 'dow',\n",
       " 'jones',\n",
       " 'industrial',\n",
       " 'average',\n",
       " 'céder',\n",
       " 'nasdaq',\n",
       " 'mercredi',\n",
       " 'rencontre',\n",
       " 'cheffe',\n",
       " 'démocrate',\n",
       " 'chambre',\n",
       " 'secrétaire',\n",
       " 'américain',\n",
       " 'trésor',\n",
       " 'discuter',\n",
       " 'nouveau',\n",
       " 'aide',\n",
       " 'économique',\n",
       " 'panne',\n",
       " 'mois',\n",
       " 'susciter',\n",
       " 'espoir',\n",
       " 'compromis',\n",
       " 'raisonnable',\n",
       " 'mot',\n",
       " 'steven',\n",
       " 'mnuchin',\n",
       " 'optimisme',\n",
       " 'donner',\n",
       " 'coup',\n",
       " 'fouet',\n",
       " 'action',\n",
       " 'brusquement',\n",
       " 'tempérer',\n",
       " 'chef',\n",
       " 'républicain',\n",
       " 'sénat',\n",
       " 'mitch',\n",
       " 'mcconnell',\n",
       " 'sortir',\n",
       " 'position',\n",
       " 'éloigné',\n",
       " 'expliquer',\n",
       " 'karl',\n",
       " 'haeling',\n",
       " 'lbbw',\n",
       " 'bourse',\n",
       " 'york',\n",
       " 'introduction',\n",
       " 'fanfare',\n",
       " 'cotation',\n",
       " 'titre',\n",
       " 'discret',\n",
       " 'groupe',\n",
       " 'surveillance',\n",
       " 'donnée',\n",
       " 'palantir',\n",
       " 'prix',\n",
       " 'valoriser',\n",
       " 'milliard',\n",
       " 'dollar',\n",
       " 'symbole',\n",
       " 'pltr',\n",
       " 'titre',\n",
       " 'clôturer',\n",
       " 'dollar',\n",
       " 'prix',\n",
       " 'indicatif',\n",
       " 'dollar',\n",
       " 'donner',\n",
       " 'mardi',\n",
       " 'soir',\n",
       " 'new',\n",
       " 'york',\n",
       " 'stock',\n",
       " 'lire',\n",
       " 'page',\n",
       " 'titre',\n",
       " 'fabricant',\n",
       " 'camion',\n",
       " 'électrique',\n",
       " 'hydrogène',\n",
       " 'nikola',\n",
       " 'reprendre',\n",
       " 'vigueur',\n",
       " 'dollar',\n",
       " 'descente',\n",
       " 'enfer',\n",
       " 'marquer',\n",
       " 'perte',\n",
       " 'tiers',\n",
       " 'valeur',\n",
       " 'introduction',\n",
       " 'bourse',\n",
       " 'nikola',\n",
       " 'ajourner',\n",
       " 'mercredi',\n",
       " 'événement',\n",
       " 'cours',\n",
       " 'devoir',\n",
       " 'présenter',\n",
       " 'grand',\n",
       " 'pompe',\n",
       " 'pick',\n",
       " 'up',\n",
       " 'badger',\n",
       " 'quasiment',\n",
       " 'secteur',\n",
       " 's&p',\n",
       " 'terminer',\n",
       " 'vert',\n",
       " 'santé',\n",
       " 'tête',\n",
       " 'laboratoire',\n",
       " 'pfizer',\n",
       " 'prendre',\n",
       " 'grand',\n",
       " 'nom',\n",
       " 'tech',\n",
       " 'progresser',\n",
       " 'microsoft',\n",
       " 'apple',\n",
       " 'marché',\n",
       " 'obligataire',\n",
       " 'taux',\n",
       " 'an',\n",
       " 'dette',\n",
       " 'américain',\n",
       " 'augmenter',\n",
       " 'mardi',\n",
       " 'soir']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first element of the fully preprocessed and tokenized data\n",
    "fr_tx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "7. Quantitative summary of the data cleaning\n",
    "</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words contained after data cleaning: 98303836\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of words contained after the data cleaning\n",
    "nwords_after = 0\n",
    "for article in fr_tx:\n",
    "    nwords_after = nwords_after + len(article)\n",
    "print('Total number of words contained after data cleaning:', nwords_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words per article after data cleaning: 204.30506981016788\n"
     ]
    }
   ],
   "source": [
    "# Get the average number of words per article after the data cleaning\n",
    "avg_nwords_after = nwords_after/len(fr_tx)\n",
    "avg_nwords_after\n",
    "print('Average number of words per article after data cleaning:', avg_nwords_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary variables to save RAM\n",
    "del fr_tx, fr_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the uncleaned data\n",
    "os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "fr_tx_uncleaned = pd.read_csv(\"agg_csv_sparse_fr.csv\", index_col = 0, dtype = {'so': object, 'la': object, 'tx': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for pre-cleaning:  2.4118225812911986 minutes\n",
      "Total number of words contained before data cleaning: 209660898\n"
     ]
    }
   ],
   "source": [
    "## Count the total number of words contained before the data cleaning\n",
    "# Note: to get an appropriate count of the distinct words we must at least apply the sparse preprocessing first, to ensure that all words are separated properly and distracting signs are removed\n",
    "fr_tx_uncleaned = pre_clean(fr_tx_uncleaned.tx.tolist())\n",
    "# Count the total number of words\n",
    "nwords_before = 0\n",
    "for article in fr_tx_uncleaned:\n",
    "    nwords_before = nwords_before + len(article.split())\n",
    "print('Total number of words contained before data cleaning:', nwords_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words per article before data cleaning: 435.7386867624625\n"
     ]
    }
   ],
   "source": [
    "# Get the average number of words per article before the data cleaning\n",
    "avg_nwords_before = nwords_before/len(fr_tx_uncleaned)\n",
    "print('Average number of words per article before data cleaning:', avg_nwords_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words removed by the data cleaning: 111357062\n"
     ]
    }
   ],
   "source": [
    "# Get the number of removed words\n",
    "nwords_rm = nwords_before - nwords_after\n",
    "print('Number of words removed by the data cleaning:', nwords_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.1129 percent of the words have been removed by the data cleaning.\n"
     ]
    }
   ],
   "source": [
    "# Get the ratio of the words that have been removed\n",
    "ratio_removed = nwords_rm / nwords_before\n",
    "print(np.round(ratio_removed*100,4), 'percent of the words have been removed by the data cleaning.')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89a54e7346d33656f2940aba0e47a561becf96fe36c69c035d9bc66d085d8900"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('Master_Thesis_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
