{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('Master_Thesis_env': conda)"
  },
  "interpreter": {
   "hash": "89a54e7346d33656f2940aba0e47a561becf96fe36c69c035d9bc66d085d8900"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h1 style='margin:10px 5px'> \n",
    "Master Thesis Yannik Haller - Sentiment Analysis NAIVE Classifier\n",
    "</h1>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "1. Load required packages and the data\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Hallk\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\pylab\\config.py:70: DeprecationWarning: InlineBackend._figure_formats_changed is deprecated in traitlets 4.1: use @observe and @unobserve instead.\n  def _figure_formats_changed(self, name, old, new):\n"
     ]
    }
   ],
   "source": [
    "# Import required baseline packages\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Change pandas' setting to print out long strings\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Spacy (for lemmatization)\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# TextBlob (for Sentiment Analysis)\n",
    "from textblob import Blobber\n",
    "from textblob_de import PatternTagger, PatternAnalyzer\n",
    "\n",
    "# Enable logging for gensim (optional)\n",
    "import logging\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the appropriate working directory\n",
    "os.chdir('D:\\\\Dropbox\\\\MA_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read in the fully preprocessed data\n",
    "def read_preprocessed(language, tokenize = True):\n",
    "    # Raise an error if an inadmissible language is chosen\n",
    "    allowed_languages = ['de', 'en', 'fr', 'it']\n",
    "    if language not in allowed_languages:\n",
    "        raise ValueError(\"Invalid language. Expected one of: %s\" % allowed_languages)\n",
    "    \n",
    "    # Set the appropriate working directory\n",
    "    os.chdir('D:\\\\Dropbox\\\\MA_data')\n",
    "\n",
    "    # Define the name of the file to load\n",
    "    filename = \"Preprocessed/Sentiment_Analysis/\"+language+\"_preprocessed_senti.csv\"\n",
    "\n",
    "    # Read in the dataframe containing the text data\n",
    "    tx_pp = pd.read_csv(filename, index_col = 0, dtype = {'tx': object})\n",
    "\n",
    "    # Get the articles' index together with an enumeration to identify their position in the list of precleaned articles\n",
    "    idx = tx_pp.index\n",
    "    idx = pd.DataFrame(idx, columns = [language+'_idx'])\n",
    "\n",
    "    # Reduce the dataframe to a list containing the text data\n",
    "    tx_pp = tx_pp.tx.to_list()\n",
    "\n",
    "    # Tokenize the data again if tokenize = True (RAM-saving)\n",
    "    if tokenize:\n",
    "        tx_pp = retokenize(tx_pp)\n",
    "\n",
    "    # Return the preprocessed data\n",
    "    return tx_pp, idx\n",
    "\n",
    "# Define a function to retokenize the preprocessed text data (RAM-saving)\n",
    "def retokenize(article_list):\n",
    "    for i in range(len(article_list)):\n",
    "        temp_tx = str(article_list[i]).split()\n",
    "        article_list[i] = temp_tx\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "189032"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Read in the preprocessed data (tokenized)\n",
    "it_tx, it_idx = read_preprocessed('it', tokenize = True)\n",
    "\n",
    "# Take a look at the size of the precleaned data\n",
    "sys.getsizeof(it_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['fermati',\n",
       " 'obbligare',\n",
       " 'oppresso',\n",
       " 'mondare',\n",
       " 'ginocchio',\n",
       " 'forse',\n",
       " 'applaudivamo',\n",
       " 'stadio',\n",
       " 'ora',\n",
       " 'acclamare',\n",
       " 'balcone',\n",
       " 'ricco',\n",
       " 'pallone',\n",
       " 'splendido',\n",
       " 'medico',\n",
       " 'togliere',\n",
       " 'stringere',\n",
       " 'mano',\n",
       " 'cosa',\n",
       " 'molto',\n",
       " 'baciare',\n",
       " 'contatto',\n",
       " 'solere',\n",
       " 'sfiorarsi',\n",
       " 'soffiare',\n",
       " 'vento',\n",
       " 'soli',\n",
       " 'solcare',\n",
       " 'seme',\n",
       " 'chiederci',\n",
       " 'perché',\n",
       " 'convinti',\n",
       " 'vita',\n",
       " 'non',\n",
       " 'perdere',\n",
       " 'neanche',\n",
       " 'istante',\n",
       " 'lunare',\n",
       " 'pieno',\n",
       " 'bellezza',\n",
       " 'verità',\n",
       " 'tornire',\n",
       " 'compiere',\n",
       " 'cantare',\n",
       " 'non',\n",
       " 'camminare',\n",
       " 'mai',\n",
       " 'solo',\n",
       " 'dubitiamo',\n",
       " 'esperto',\n",
       " 'nullo',\n",
       " 'lasciamo',\n",
       " 'pipistrello',\n",
       " 'maestro',\n",
       " 'biodiversità',\n",
       " 'colpa',\n",
       " 'usciamo',\n",
       " 'inferno',\n",
       " 'togliere',\n",
       " 'respirare',\n",
       " 'reagiamo',\n",
       " 'torrente',\n",
       " 'vita',\n",
       " 'dissetiamoci',\n",
       " 'fonte',\n",
       " 'naturale',\n",
       " 'sempre',\n",
       " 'convinto',\n",
       " 'amore',\n",
       " 'salvare',\n",
       " 'unico',\n",
       " 'rispondere',\n",
       " 'vincere',\n",
       " 'felicità',\n",
       " 'serenità',\n",
       " 'tornare',\n",
       " 'sicuri',\n",
       " 'dio',\n",
       " 'sconfiggere',\n",
       " 'anche',\n",
       " 'coronavirus',\n",
       " 'rodolfo',\n",
       " 'fasani',\n",
       " 'mesocco']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Take a look at the preprocessed data\n",
    "it_tx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        it_idx\n",
       "23618  2425113\n",
       "23619  2425114\n",
       "23620  2425115"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>it_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>23618</th>\n      <td>2425113</td>\n    </tr>\n    <tr>\n      <th>23619</th>\n      <td>2425114</td>\n    </tr>\n    <tr>\n      <th>23620</th>\n      <td>2425115</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Take a look at the dataframe containing the according index\n",
    "it_idx.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[23619, 23620]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Retrieve the location of the article in the preprocessed data using the according article id\n",
    "article_ids = [2425114, 2425115]\n",
    "location = it_idx[it_idx.it_idx.isin(article_ids)].index.tolist() #23619\n",
    "\n",
    "# Access the preprocessed text from the articles with the article ids in [2425114, 2425115]\n",
    "#list(it_tx[i] for i in location)\n",
    "\n",
    "# Look at the according location of the articles with the article ids in [2425114, 2425115]\n",
    "location"
   ]
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "2. Sentiment assessment of the articles\n",
    "</h2>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17647 exactly duplicated entries have been removed.\n"
     ]
    }
   ],
   "source": [
    "# Read in the sentiment lexicon Sentix (i.e. an Italian lexicon for sentiment analysis) as a dataframe\n",
    "senti_lex_df = pd.read_csv(\"Sentiment/Naive/Italian/sentix.txt\", sep = '\\t', header = None, names = ['lemma','POS','ID','pos_score','neg_score','polarity','intensity'])\n",
    "# Lowercase the entries in the column 'lemma'\n",
    "senti_lex_df['lemma'] = senti_lex_df['lemma'].str.lower()\n",
    "# Relabel the POS Tags to a common standard\n",
    "senti_lex_df['POS'].replace({'a': 'ADJ', 'n': 'NOUN', 'v': 'VERB', 'r': 'ADV'}, inplace = True)\n",
    "# Remove exact duplicates\n",
    "n_duplicates = sum(senti_lex_df.duplicated())\n",
    "senti_lex_df.drop_duplicates(keep = 'first', inplace = True, ignore_index = True)\n",
    "print(n_duplicates, \"exactly duplicated entries have been removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "675 duplicated lemmas with differing POS-tags have been removed.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average polarity of all duplicated words that are assigned with the same POS\n",
    "senti_lex_df = senti_lex_df.groupby(['POS','lemma'])['polarity'].mean().reset_index()\n",
    "# Sort the dataframe according to the alphabetical order of the POS-tags, such that first all ADJs appear, which then are followed by ADVs, NOUNs and then VERBs\n",
    "senti_lex_df.sort_values(['POS','lemma'], inplace = True)\n",
    "# Remove duplicates, while the first appearing lemma is kept (hence, in case of duplicated lemmas that have different POS-tags, first adjectives are kept, then adverbs, then nouns and then verbs)\n",
    "n_duplicates = sum(senti_lex_df.duplicated(subset = ['lemma'], keep = 'first'))\n",
    "print(n_duplicates, \"duplicated lemmas with differing POS-tags have been removed.\")\n",
    "senti_lex_df.drop_duplicates(subset = ['lemma'], keep = 'first', inplace = True, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        POS                          lemma  polarity\n",
       "0       ADJ                              1 -1.000000\n",
       "1       ADJ                  a_bassa_quota -1.000000\n",
       "2       ADJ            a_bassa_risoluzione  1.000000\n",
       "3       ADJ  a_basso_contenuto_tecnologico  1.000000\n",
       "4       ADJ                a_basso_livello -1.000000\n",
       "...     ...                            ...       ...\n",
       "41794  VERB                     voler_bene  0.688083\n",
       "41795  VERB                        volerci  1.000000\n",
       "41796  VERB                      vulnerare -1.000000\n",
       "41797  VERB                        zoomare  1.000000\n",
       "41798  VERB                         zumare  1.000000\n",
       "\n",
       "[41799 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>POS</th>\n      <th>lemma</th>\n      <th>polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ADJ</td>\n      <td>1</td>\n      <td>-1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ADJ</td>\n      <td>a_bassa_quota</td>\n      <td>-1.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ADJ</td>\n      <td>a_bassa_risoluzione</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ADJ</td>\n      <td>a_basso_contenuto_tecnologico</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ADJ</td>\n      <td>a_basso_livello</td>\n      <td>-1.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41794</th>\n      <td>VERB</td>\n      <td>voler_bene</td>\n      <td>0.688083</td>\n    </tr>\n    <tr>\n      <th>41795</th>\n      <td>VERB</td>\n      <td>volerci</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>41796</th>\n      <td>VERB</td>\n      <td>vulnerare</td>\n      <td>-1.000000</td>\n    </tr>\n    <tr>\n      <th>41797</th>\n      <td>VERB</td>\n      <td>zoomare</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>41798</th>\n      <td>VERB</td>\n      <td>zumare</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>41799 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Take a look at the dataframe containing the sentiment lexicon\n",
    "senti_lex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary out of the sentiment lexicon\n",
    "senti_lex_dict = {}\n",
    "for index, row in senti_lex_df.iterrows():\n",
    "    senti_lex_dict[row['lemma']] = {'POS': str(row['POS']), 'polarity': float(row['polarity'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary variables to save RAM\n",
    "del senti_lex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the set of possible negations\n",
    "negations = ['no', 'non', 'niente', 'nessuno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirically derived mean sentiment intensity rating increase for booster words (adapted from the VADER module)\n",
    "# Note: The values have been devided by 4, because we are working with polarities directly (which range from -1 to 1) instead of the unscaled crowd ratings (which range from -4 to 4)\n",
    "B_INCR = 0.293/4\n",
    "B_DECR = -0.293/4\n",
    "\n",
    "# Define the dictionary of booster words\n",
    "booster_dic = \\\n",
    "    {\"assolutamente\": B_INCR, \"assoluto\": B_INCR, \"assoluta\": B_INCR, \"totalmente\": B_INCR, \"totale\": B_INCR, #\"absolutely\": B_INCR,\n",
    "     \"sorprendente\": B_INCR, \"mirabolante\": B_INCR, \"stupefacente\": B_INCR, \"straordinario\": B_INCR, \"straordinaria\": B_INCR, \"strabiliante\": B_INCR, #\"amazingly\": B_INCR,\n",
    "     \"enorme\": B_INCR, \"esorbitante\": B_INCR, \"immenso\": B_INCR, \"immensa\": B_INCR, \"colossale\": B_INCR, #\"awfully\": B_INCR,\n",
    "     \"completo\": B_INCR, \"completa\": B_INCR, \"intero\": B_INCR, \"intera\": B_INCR,    #\"completely\": B_INCR,\n",
    "     \"considerevole\": B_INCR, \"ingente\": B_INCR, \"notevole\": B_INCR, \"ragguardevole\": B_INCR, \"rilevante\": B_INCR, \"apprezzabile\": B_INCR, \"cospicuo\": B_INCR, \"cospicua\": B_INCR, #\"considerably\": B_INCR,\n",
    "     \"inequivocabile\": B_INCR, \"univoco\": B_INCR, \"univoca\": B_INCR, \"netto\": B_INCR, \"netta\": B_INCR, \"indubitato\": B_INCR, \"indubitata\": B_INCR, #\"decidedly\": B_INCR,\n",
    "     \"fundamentale\": B_INCR, #\"deeply\": B_INCR,\n",
    "     \"dannato\": B_INCR, \"dannata\": B_INCR, #\"effing\": B_INCR,\n",
    "     \"oltremodo\": B_INCR, \"oltremisura\": B_INCR, \"sommamente\": B_INCR, \"squisitamente\": B_INCR, \"straordinariamente\": B_INCR, #\"enormously\": B_INCR,\n",
    "     #\"entirely\": B_INCR,\n",
    "     \"particolare\": B_INCR, \"particolarmente\": B_INCR, \"speciale\": B_INCR, \"specialmente\": B_INCR, # \"especially\": B_INCR,\n",
    "     \"insolito\": B_INCR, \"insolita\": B_INCR, \"eccezionalmente\": B_INCR,  #\"exceptionally\": B_INCR,\n",
    "     \"estremamente\": B_INCR, \"estremo\": B_INCR, \"estrema\": B_INCR, #\"extremely\": B_INCR,\n",
    "     \"favoloso\": B_INCR, \"favolosa\": B_INCR, \"fantastico\": B_INCR, #\"fabulously\": B_INCR,\n",
    "     #\"flipping\": B_INCR,\n",
    "     #\"flippin\": B_INCR,\n",
    "     #\"fricking\": B_INCR,\n",
    "     #\"frickin\": B_INCR,\n",
    "     #\"frigging\": B_INCR,\n",
    "     #\"friggin\": B_INCR,\n",
    "     #\"fully\": B_INCR,\n",
    "     #\"fucking\": B_INCR,\n",
    "     \"molto\": B_INCR, \"intensamente\": B_INCR, \"parecchio\": B_INCR, \"tanto\": B_INCR, \"massiccio\": B_INCR, \"massiccia\": B_INCR,  #\"greatly\": B_INCR,\n",
    "     #\"hella\": B_INCR,\n",
    "     \"supremo\": B_INCR, \"suprema\": B_INCR, #\"highly\": B_INCR,\n",
    "     \"immensamente\": B_INCR, \"immenso\": B_INCR, \"immensa\": B_INCR, #\"hugely\": B_INCR,\n",
    "     \"incredibile\": B_INCR, #\"incredibly\": B_INCR,\n",
    "     \"intensamente\": B_INCR, #\"intensely\": B_INCR,\n",
    "     \"principalmente\": B_INCR, #\"majorly\": B_INCR,\n",
    "     \"più\": B_INCR, #\"more\": B_INCR,\n",
    "     \"maggior\": B_INCR, #\"most\": B_INCR,\n",
    "     \"particolarmente\": B_INCR, \"soprattutto\": B_INCR, #\"particularly\": B_INCR,\n",
    "     \"puramente\": B_INCR, \"esclusivamente\": B_INCR, #\"purely\": B_INCR,\n",
    "     \"abbastanza\": B_INCR, \"piuttosto\": B_INCR, \"alquanto\": B_INCR, #\"quite\": B_INCR,\n",
    "     \"davvero\": B_INCR, \"veramente\": B_INCR, #\"really\": B_INCR,\n",
    "     \"notevolmente\": B_INCR, #\"remarkably\": B_INCR,\n",
    "     \"essenziale\": B_INCR, \"considerabilmente\": B_INCR, #\"substantially\": B_INCR,\n",
    "     \"accuratamente\": B_INCR, \"completamente\": B_INCR, #\"thoroughly\": B_INCR,\n",
    "     #\"totally\": B_INCR,\n",
    "     \"tremendamente\": B_INCR, \"enormemente\": B_INCR, #\"tremendously\": B_INCR,\n",
    "     #\"uber\": B_INCR,\n",
    "     \"incredibilmente\": B_INCR, #\"unbelievably\": B_INCR,\n",
    "     \"insolitamente\": B_INCR, \"inusualmente\": B_INCR, #\"unusually\": B_INCR,\n",
    "     #\"utterly\": B_INCR,\n",
    "     #\"very\": B_INCR,\n",
    "     #####\n",
    "     \"quasi\": B_DECR, \"pressoché\": B_DECR, #\"almost\": B_DECR,\n",
    "     \"appena\": B_INCR, \"malapena\": B_INCR, #\"barely\": B_DECR,\n",
    "     \"stento\": B_DECR, #\"hardly\": B_DECR,\n",
    "     \"abbastanza\": B_INCR, #\"just enough\": B_DECR,\n",
    "     \"alquanto\": B_DECR, #\"kind of\": B_DECR,\n",
    "     \"tipo\": B_INCR, #\"kinda\": B_DECR,\n",
    "     #\"kindof\": B_DECR,\n",
    "     #\"kind-of\": B_DECR,\n",
    "     \"meno\": B_INCR, #\"less\": B_DECR,\n",
    "     \"piccolo\": B_INCR, #\"little\": B_DECR,\n",
    "     \"esiguo\": B_DECR, \"esigua\": B_DECR, \"futile\": B_DECR, \"insignificante\": B_DECR, \"marginale\": B_DECR,  #\"marginally\": B_DECR,\n",
    "     \"occasionale\": B_DECR, \"saltuario\": B_DECR, \"saltuaria\": B_DECR, #\"occasionally\": B_DECR,\n",
    "     \"parziale\": B_DECR, #\"partly\": B_DECR,\n",
    "     \"scarso\": B_DECR, \"scarsa\": B_DECR, \"rado\": B_DECR, \"rada\": B_DECR, \"scarsamente\": B_DECR, \"magro\": B_DECR, \"magra\": B_DECR, #\"scarcely\": B_DECR,\n",
    "     \"poco\": B_DECR, \"briciolo\": B_DECR, \"pizzico\": B_DECR, #\"slightly\": B_DECR,\n",
    "     \"piuttosto\": B_DECR #\"somewhat\": B_DECR,\n",
    "     #\"sort of\": B_DECR,\n",
    "     #\"sorta\": B_DECR,\n",
    "     #\"sortof\": B_DECR,\n",
    "     #\"sort-of\": B_DECR\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Sentiment Classifier class\n",
    "class NaiveSentimentClassifierIT:\n",
    "    def __init__(self, senti_lex_dict, negations, booster_dic):\n",
    "        # Note:\n",
    "        ## senti_lex_dict has to be a dictionary with entries of the following form: {token: {'POS': token_POStag, 'polarity': token_polarity_score}}\n",
    "        ## negations has to be a list of negation words\n",
    "        ## booster_dic has to be a dictionary with entries of the following form: {token: additive_polarity_impact_on_target_token}\n",
    "        # Store the inputs within the corresponding attribute of self\n",
    "        self.senti_lex_dict = senti_lex_dict\n",
    "        self.negations      = negations\n",
    "        self.booster_dic    = booster_dic\n",
    "    \n",
    "    # Set up a function that evaluates the polarity of the articles\n",
    "    def evaluate(self, tx, idx, name_output_file = 'it_naive_polarity'):\n",
    "        # Note: \n",
    "        ## tx has to be a list of tokenized articles (i.e. a list, whose elements are itself lists containing the tokenized and precleaned articles)\n",
    "        ## --> (precleaned means lemmatized and filtered, such that only negations, nouns, verbs, adverbs and adjectives are contained)\n",
    "        ## idx has to be a list containing the ordered article indexes corresponding to the articles in tx\n",
    "\n",
    "        # Keep track of the processing time\n",
    "        t = time.time()\n",
    "\n",
    "        # Create an empty list to store the resulting document polarity scores\n",
    "        article_polarity = []\n",
    "        # Set up a loop to go through all articles\n",
    "        for article in tx:\n",
    "            # Apply the above defined functions to get the token's polarity scores, score adjustments (through booster words) and score multipliers (through negations)\n",
    "            self.get_token_postag(article)\n",
    "            self.get_token_polarity(article)\n",
    "            self.get_score_adjustment(article)\n",
    "            self.get_score_multiplier(article)\n",
    "            self.get_article_polarity()\n",
    "            # Apply the above defined function to calculate the final polarity of the article and append it to the variable article_polarity\n",
    "            article_polarity.append(self.document_polarity)\n",
    "        # Store the article polarities in self\n",
    "        self.article_polarity = article_polarity\n",
    "\n",
    "        # Print out the processing time\n",
    "        print(\"Processing time to evaluate the article sentiments:\", str((time.time() - t)/60), \"minutes\")\n",
    "\n",
    "        # Create a correctly indexed dataframe containing the article sentiments\n",
    "        Naive_tx_polarity = pd.DataFrame(article_polarity, index = idx, columns = ['Naive_polarity'])\n",
    "        # Save the results to a csv file\n",
    "        Naive_tx_polarity.to_csv(\"Sentiment/Naive/\"+name_output_file+\".csv\", index = True)\n",
    "        # Return the results\n",
    "        return Naive_tx_polarity\n",
    "\n",
    "    ## Define all functions needed within the Sentiment Classifier class\n",
    "\n",
    "    # Define a function to get the POS-tag for each token in an article (given that the token is contained in the sentiment lexicon)\n",
    "    def get_token_postag(self, article):\n",
    "        # Create a list to store the results\n",
    "        token_pos = []\n",
    "        # Set up a loop to go through all tokens of the article\n",
    "        for token in article:\n",
    "            if token in self.senti_lex_dict:\n",
    "                token_pos.append(self.senti_lex_dict[token]['POS'])\n",
    "            else:\n",
    "                token_pos.append('UNKNOWN')\n",
    "        # Store the resulting list of the token POS-tags in self\n",
    "        self.token_pos = token_pos\n",
    "\n",
    "    # Define a function to get the polarity score for each token in an article (given that the token is contained in the sentiment lexicon)\n",
    "    def get_token_polarity(self, article):\n",
    "        # Create a list to store the results\n",
    "        token_polarity = []\n",
    "        # Set up a loop to go through all tokens of the article\n",
    "        for token in article:\n",
    "            if token in self.senti_lex_dict:\n",
    "                token_polarity.append(self.senti_lex_dict[token]['polarity'])\n",
    "            else:\n",
    "                token_polarity.append(0)\n",
    "        # Store the resulting list of the token polarities in self\n",
    "        self.token_polarity = token_polarity\n",
    "\n",
    "    # Define a function to get the score multiplier (caused by negation words) for each token in an article\n",
    "    def get_score_multiplier(self, article):\n",
    "        # Define the negation scalar (adapted from VADER)\n",
    "        neg_scalar = -0.74\n",
    "        # Define a list of ones of the same length as the number of tokens in the article\n",
    "        score_multiplier = np.repeat(1, len(article)).tolist()\n",
    "        # Set up a loop to go through all tokens of the article\n",
    "        for i in np.arange(1, len(article)-1):\n",
    "            # Check whether the word is a negation word and assign a neg_scalar to the multiplier at the position of the subsequnet token and a 0.5*neg_scalar at the position of the second token after the negation\n",
    "            if article[i] in self.negations:\n",
    "                score_multiplier[i+1] = neg_scalar\n",
    "                if i < (len(article)-2):\n",
    "                   score_multiplier[i+2] = 0.5*neg_scalar\n",
    "        # Store the score_multiplier variable in self\n",
    "        self.score_multiplier = score_multiplier\n",
    "\n",
    "    # Define a function to get the score adjustment (caused by intensifier words) for each token in an article\n",
    "    def get_score_adjustment(self, article):\n",
    "        # Define a list of zeros of the same length as the number of tokens in the article\n",
    "        score_adjustment = np.repeat(0, len(article)).tolist()\n",
    "        # Set up a loop to go through all tokens of the article\n",
    "        for i in np.arange(1, len(article)-1):\n",
    "            # If the previous word was a verb and the current word is a booster, then assign an adjustment to the verb\n",
    "            if article[i] in self.booster_dic and self.token_pos[i-1] == 'VERB':\n",
    "                score_adjustment[i-1] = score_adjustment[i-1] + self.booster_dic[article[i]]\n",
    "            # Else, it is assumed that the booster affects the subsequent token\n",
    "            elif article[i] in self.booster_dic and not self.token_pos[i-1] == 'VERB':\n",
    "                score_adjustment[i+1] = score_adjustment[i+1] + self.booster_dic[article[i]]\n",
    "        # Store the score_adjustment variable in self\n",
    "        self.score_adjustment = score_adjustment\n",
    "\n",
    "    # Define a function to calculate the final polarity of an article\n",
    "    def get_article_polarity(self):\n",
    "        # Create an empty list to store the final token polarity (note: from now on only tokens with a polarity != 0 are kept)\n",
    "        final_token_polarity = []\n",
    "        # Set up a loop to calculate each token's final polarity score\n",
    "        for i in range(len(self.token_polarity)):\n",
    "            if self.token_polarity[i] > 0:\n",
    "                final_token_polarity.append((self.token_polarity[i] + self.score_adjustment[i])*self.score_multiplier[i])\n",
    "            if self.token_polarity[i] < 0:\n",
    "                final_token_polarity.append((self.token_polarity[i] - self.score_adjustment[i])*self.score_multiplier[i])\n",
    "        # Calculate the article polarity, which is just the average final token polarity among all tokens that were kept (i.e. the tokens that exhibit a non-zero final polarity score)\n",
    "        # If the list final_token_polarity is empty, then just assign a polarity of 0\n",
    "        if len(final_token_polarity) == 0:\n",
    "            document_polarity = 0\n",
    "        else:\n",
    "            document_polarity = np.mean(final_token_polarity)\n",
    "        # Ensure that the resulting polarity is still in the range between -1 and 1\n",
    "        if document_polarity > 1: document_polarity = 1\n",
    "        if document_polarity < -1: document_polarity = -1\n",
    "        # Store the resulting document polarity in self\n",
    "        self.document_polarity = document_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing time to evaluate the article sentiments: 0.07300450007120768 minutes\n"
     ]
    }
   ],
   "source": [
    "# Set up a NaiveSentimentClassifierIT object\n",
    "NSC_it = NaiveSentimentClassifierIT(senti_lex_dict, negations, booster_dic)\n",
    "# Evaluate the sentiment of the Italian articles\n",
    "Naive_tx_polarity = NSC_it.evaluate(it_tx, it_idx.it_idx.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Naive_polarity\n",
       "313578         0.254330\n",
       "460527         0.066695\n",
       "460528         0.027784\n",
       "460529         0.375276\n",
       "460530         0.077931\n",
       "...                 ...\n",
       "2425111        0.325523\n",
       "2425112        0.290054\n",
       "2425113        0.139710\n",
       "2425114       -0.018780\n",
       "2425115       -0.094737\n",
       "\n",
       "[23621 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Naive_polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>313578</th>\n      <td>0.254330</td>\n    </tr>\n    <tr>\n      <th>460527</th>\n      <td>0.066695</td>\n    </tr>\n    <tr>\n      <th>460528</th>\n      <td>0.027784</td>\n    </tr>\n    <tr>\n      <th>460529</th>\n      <td>0.375276</td>\n    </tr>\n    <tr>\n      <th>460530</th>\n      <td>0.077931</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2425111</th>\n      <td>0.325523</td>\n    </tr>\n    <tr>\n      <th>2425112</th>\n      <td>0.290054</td>\n    </tr>\n    <tr>\n      <th>2425113</th>\n      <td>0.139710</td>\n    </tr>\n    <tr>\n      <th>2425114</th>\n      <td>-0.018780</td>\n    </tr>\n    <tr>\n      <th>2425115</th>\n      <td>-0.094737</td>\n    </tr>\n  </tbody>\n</table>\n<p>23621 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Take a look at the results\n",
    "Naive_tx_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The share of articles with a positive sentiment is 88.0 %\nThe share of articles with a negative sentiment is 12.0 %\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Naive_polarity\n",
       "count       23621.000\n",
       "mean            0.209\n",
       "std             0.198\n",
       "min            -0.867\n",
       "25%             0.093\n",
       "50%             0.210\n",
       "75%             0.325\n",
       "max             1.000"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Naive_polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>23621.000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.209</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.198</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-0.867</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.093</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.210</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.325</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Take a look at some summary statistics\n",
    "share_pos = np.round(np.sum(Naive_tx_polarity['Naive_polarity'] > 0) / len(Naive_tx_polarity),2)\n",
    "share_neg = np.round(np.sum(Naive_tx_polarity['Naive_polarity'] < 0) / len(Naive_tx_polarity),2)\n",
    "print('The share of articles with a positive sentiment is', 100*share_pos,'%')\n",
    "print('The share of articles with a negative sentiment is', 100*share_neg,'%')\n",
    "np.round(Naive_tx_polarity.describe(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the results back in\n",
    "Naive_tx_polarity = pd.read_csv(\"Sentiment/Naive/it_naive_polarity.csv\", index_col = 0, dtype = {'Naive_polarity': float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Naive_polarity\n",
       "313578         0.254330\n",
       "460527         0.066695\n",
       "460528         0.027784\n",
       "460529         0.375276\n",
       "460530         0.077931\n",
       "...                 ...\n",
       "2425111        0.325523\n",
       "2425112        0.290054\n",
       "2425113        0.139710\n",
       "2425114       -0.018780\n",
       "2425115       -0.094737\n",
       "\n",
       "[23621 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Naive_polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>313578</th>\n      <td>0.254330</td>\n    </tr>\n    <tr>\n      <th>460527</th>\n      <td>0.066695</td>\n    </tr>\n    <tr>\n      <th>460528</th>\n      <td>0.027784</td>\n    </tr>\n    <tr>\n      <th>460529</th>\n      <td>0.375276</td>\n    </tr>\n    <tr>\n      <th>460530</th>\n      <td>0.077931</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2425111</th>\n      <td>0.325523</td>\n    </tr>\n    <tr>\n      <th>2425112</th>\n      <td>0.290054</td>\n    </tr>\n    <tr>\n      <th>2425113</th>\n      <td>0.139710</td>\n    </tr>\n    <tr>\n      <th>2425114</th>\n      <td>-0.018780</td>\n    </tr>\n    <tr>\n      <th>2425115</th>\n      <td>-0.094737</td>\n    </tr>\n  </tbody>\n</table>\n<p>23621 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# Take a look at the read in results\n",
    "Naive_tx_polarity"
   ]
  },
  {
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> \n",
    "3. Extend the underlying sentiment lexicon with common covid-related words\n",
    "</h2>\n",
    "</div>\n",
    "\n",
    "According to the naïve polarity scores estimated in the previous section most covid related newspaper articles appear to convey a slightly positive sentiment. However, this observation is against our expectations and we therefore try to figure out what might be driving these sentiment classification outcomes to be predominantly positive in the following. Since the self developped naïve sentiment classifier relies on a lexicon based approach, inspecting the underlying sentiment lexicon (i.e. the sentix lexicon) seems a good starting point thereunto. As such, we check whether the following phrases (which - partly intuitively and partly according to the main LDA topic models - are strongly indicative for an article to be covid-related) are captured by the original sentix lexicon:\n",
    "\n",
    "- 'covid' ($\\rightarrow$ 'covid' in Italian)\n",
    "- 'corona' ($\\rightarrow$ 'covid'/'coronavirus' in Italian)\n",
    "- 'coronavirus' ($\\rightarrow$ 'coronavirus' in Italian)\n",
    "- 'virus' ($\\rightarrow$ 'virus' in Italian)\n",
    "- 'infection' ($\\rightarrow$ 'infezione'/'contagio'/'contaminazione' in Italian)\n",
    "- 'infect' ($\\rightarrow$ 'infettare'/'contagiare' in Italian)\n",
    "- 'infected ($\\rightarrow$ 'infettato' in Italian)\n",
    "- 'infectious' ($\\rightarrow$ 'infettivo'/'contagioso'/'virulento' in Italian)\n",
    "- 'pandemic' ($\\rightarrow$ 'pandemia' in Italian)\n",
    "- 'epidemic' ($\\rightarrow$ 'epidemia' in Italian)\n",
    "- 'lockdown' ($\\rightarrow$ 'lockdown'/'coprifuoco' in Italian)\n",
    "- 'crisis' ($\\rightarrow$ 'crisi' in Italian)\n",
    "- 'quarantine' ($\\rightarrow$ 'quarantena'/'contumacia' in Italian)\n",
    "- 'hospitalisation' ($\\rightarrow$ 'ricovero' in Italian)\n",
    "- 'disease' ($\\rightarrow$ 'malattia'/'infermità'/'morbo' in Italian)\n",
    "\n",
    "Searching these words in the original sentix lexicon reveals that most of them are not contained, meaning that most words in this list are not assigned an appropriate polarity score and therefore do not influence the overall polarity of the articles in which they appear. Since for all words in the above list it can reasonably be argued that they rather should convey a negative feeling, omitting them results in positively biased polarity scores for articles that contain them. Thus, we chase the following strategy to account for this issue: for each word listed above, we check whether the Italian equivalent or close synonyms of it are contained in the original sentix lexicon in a first step. Thus, the following cases may occur:\n",
    "\n",
    " - Case 1 - The word itself as well as close synonyms of it are contained: in this case we check whether the polarity score assigned to the focal word is negative and, if yes, leave the entry unchanged. Otherwise, we proceed as described in case 3.\n",
    "\n",
    " - Case 2 - The word itself is contained, but no close synonyms of it: in this case we check whether the polarity score assigned to the focal word is negative and, if yes, leave the entry unchanged. Otherwise, we proceed as described in case 4.\n",
    "\n",
    " - Case 3 - A close synonym of the word is contained, but the word itself is not: in this case we check whether the polarity score assigned to the synonym is negative and, if yes, assign the same polarity score to the focal word. Otherwise, if the synonym's polarity score is positive, we proceed as described in case 4.\n",
    "\n",
    " - Case 4 - Neither the word itself nor close synonyms of it are contained: in this case we start by checking whether the word's primary word (or a synonym's primary word) is contained in the lexicon and, if yes, assign the same polarity score to the focal word. Otherwise, if the primary word is not contained, we continue by checking whether the German/French equivalent of the word is contained in the German/French Vader sentiment lexicon. If yes, we transform the valence score observed there into a polarity score (by dividing it by 4) and assign this score to the focal Italian word. If this approach also fails, we add the word and its synonyms to the lexicon and assign a polarity score of -1 (note: adding grammatical cases (e.g. dative, genitive) or conjugations theoretically is not necessary here, because the Italian articles are lemmatized before they are passed to the sentiment algorithm. However, since the Italian spacy implemenation used for lemmatizing does not seem to always work properly, we decide to add at least the male and female forms of adjectives).\n",
    "\n",
    "According to this strategy, we apply the following editings to the sentix lexicon:\n",
    "\n",
    "- 'covid' ($\\rightarrow$ 'covid' in Italian): not contained $\\rightarrow$ assigned with a polarity score of -1 (added cases: 'covid')\n",
    "- 'corona' ($\\rightarrow$ 'covid'/'coronavirus' in Italian): 'covid' not contained $\\rightarrow$ assigned with a polarity score of -1 (added cases: 'covid') / 'coronavirus' not contained $\\rightarrow$ assigned with a polarity score of -1 (added cases: 'coronavirus')\n",
    "- 'coronavirus' ($\\rightarrow$ 'coronavirus' in Italian): not contained $\\rightarrow$ assigned with a polarity score of -1 (added cases: 'coronavirus')\n",
    "- 'virus' ($\\rightarrow$ 'virus' in Italian): already contained, but multiple entries observed with a polarity scores of either -1 or 1 $\\rightarrow$ removed all entries except one which exhibits a polarity score assignment of -1 (note: before this adjustment the polarity assigned to the word 'virus' was 0)\n",
    "- 'infection' ($\\rightarrow$ 'infezione'/'contagio'/'contaminazione' in Italian): 'contagio' already contained with a polarity score of 1 $\\rightarrow$ re-assigned with a polarity score of -0.5 as suggested by its French equivalent 'contagion' / 'infezione' already contained with a polarity score of 0 $\\rightarrow$ re-assigned with a polarity score of -0.5 since it is a close synonym of 'contagio' / 'contaminazione' not contained $\\rightarrow$ assigned with a polarity score of -0.5 since it is a close synonym of 'contagio' (added cases: 'contaminazione')\n",
    "- 'infect' ($\\rightarrow$ 'infettare'/'contagiare' in Italian): 'infettare' already contained with a polarity score of 0.41 $\\rightarrow$ re-assigned with a polarity score of -0.5 since 'infezione' is its primary word / 'contagiare' already contained with a polarity score of 0.41 $\\rightarrow$ re-assigned with a polarity score of -0.5 since 'contagio' is its primary word\n",
    "- 'infected ($\\rightarrow$ 'infettato' in Italian): not contained $\\rightarrow$ assigned with a polarity score of -0.5 since 'infezione' is its primary word (added cases: 'infettato', 'infettata')\n",
    "- 'infectious' ($\\rightarrow$ 'infettivo'/'contagioso'/'virulento' in Italian): 'infettivo' already contained with a polarity score of 1 $\\rightarrow$ re-assigned with a polarity score of -0.5 since 'infezione' is its primary word (note: added female case 'infettiva') / 'contagioso' already contained with a polarity score of 1 $\\rightarrow$ re-assigned with a polarity score of -0.5 since 'contagio' is its primary word (note: added female case 'contagiosa')/ 'virulento' already contained with a polarity score of 1 $\\rightarrow$ re-assigned with a polarity score of -0.5 since it is a close synonym of 'contagioso' (note: added female case 'virulenta')\n",
    "- 'pandemic' ($\\rightarrow$ 'pandemia' in Italian): not contained $\\rightarrow$ assigned with a polarity score of -0.675 as suggested by its German equivalent 'Pandemie' (added cases: 'pandemia')\n",
    "- 'epidemic' ($\\rightarrow$ 'epidemia' in Italian): not contained $\\rightarrow$ assigned with a polarity score of -0.675 as suggested by its German equivalent 'Epidemie' (added cases: 'epidemia')\n",
    "- 'lockdown' ($\\rightarrow$ 'lockdown'/'coprifuoco' in Italian): 'lockdown' not contained $\\rightarrow$ assigned with a polarity score of -1 (added cases: 'lockdown') / 'coprifuoco' not contained $\\rightarrow$ assigned with a polarity score of -1 (added cases: 'coprifuoco')\n",
    "- 'crisis' ($\\rightarrow$ 'crisi' in Italian): already contained, but multiple entries observed with a polarity scores of either -1 (2 distinct entries) or -0.25 (1 entry repeated three times) $\\rightarrow$ removed all entries except one and re-assigned it with a polarity score of -0.75, which is the average polarity score of the 3 distinct original entries (note: this coincides with the polarity score used by the algorithm before this adjustment was made)\n",
    "- 'quarantine' ($\\rightarrow$ 'quarantena'/'contumacia' in Italian): 'quarantena' already contained with a polarity score of 1 $\\rightarrow$ re-assigned with a polarity score of -1 / 'contumacia' not contained $\\rightarrow$ assigned with a polarity score of -1 since it is a close synonym of 'quarantena' (added cases: 'contumacia')\n",
    "- 'hospitalisation' ($\\rightarrow$ 'ricovero' in Italian): already contained, but multiple entries observed with a polarity scores of either -1 (1 entry) or 0 (1 entry) $\\rightarrow$ removed all entries except one and re-assigned it with a polarity score of -0.5, which is the average polarity score of the 2 distinct original entries (note: this coincides with the polarity score used by the algorithm before this adjustment was made)\n",
    "- 'disease' ($\\rightarrow$ 'malattia'/'infermità'/'morbo' in Italian): 'malattia' already contained, but multiple entries observed with a polarity scores of -1 (4 distinct entries), -0.41 (1 entry repeated 3 times), 0.25 (1 entry repeated 4 times) or 1 (2 distinct entries) $\\rightarrow$ removed all entries except one and re-assigned it with a polarity score of -0.27, which is the average polarity score of the 2 distinct original entries (note: this coincides with the polarity score used by the algorithm before this adjustment was made) / 'infermità' already contained with a polarity score of 1 $\\rightarrow$ re-assigned with a polarity score of -0.27 since it is a close synonym of 'malattia' / 'morbo' already contained, but multiple entries observed with a polarity scores of either 0.25 (1 entry) or 1 (1 entry) $\\rightarrow$ removed all entries except one and re-assigned it with a polarity score of -0.27 since it is a close synonym of 'malattia'\n",
    "\n",
    "After adjusting the sentiment lexicon as desired we use it to rerun the sentiment analysis. To do so, we have to run the subsequent codes (assuming that at least the first 14 code chunks of this Jupyter notebook have been executed beforehand)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17631 exactly duplicated entries have been removed.\n"
     ]
    }
   ],
   "source": [
    "# Read in the manually extended Sentix sentiment lexicon (i.e. an Italian lexicon for sentiment analysis) as a dataframe\n",
    "senti_lex_df = pd.read_csv(\"Sentiment/Naive/Italian/sentix_extended.txt\", sep = '\\t', header = None, names = ['lemma','POS','ID','pos_score','neg_score','polarity','intensity'])\n",
    "# Lowercase the entries in the column 'lemma'\n",
    "senti_lex_df['lemma'] = senti_lex_df['lemma'].str.lower()\n",
    "# Relabel the POS Tags to a common standard\n",
    "senti_lex_df['POS'].replace({'a': 'ADJ', 'n': 'NOUN', 'v': 'VERB', 'r': 'ADV'}, inplace = True)\n",
    "# Remove exact duplicates\n",
    "n_duplicates = sum(senti_lex_df.duplicated())\n",
    "senti_lex_df.drop_duplicates(keep = 'first', inplace = True, ignore_index = True)\n",
    "print(n_duplicates, \"exactly duplicated entries have been removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "673 duplicated lemmas with differing POS-tags have been removed.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average polarity of all duplicated words that are assigned with the same POS\n",
    "senti_lex_df = senti_lex_df.groupby(['POS','lemma'])['polarity'].mean().reset_index()\n",
    "# Sort the dataframe according to the alphabetical order of the POS-tags, such that first all ADJs appear, which then are followed by ADVs, NOUNs and then VERBs\n",
    "senti_lex_df.sort_values(['POS','lemma'], inplace = True)\n",
    "# Remove duplicates, while the first appearing lemma is kept (hence, in case of duplicated lemmas that have different POS-tags, first adjectives are kept, then adverbs, then nouns and then verbs)\n",
    "n_duplicates = sum(senti_lex_df.duplicated(subset = ['lemma'], keep = 'first'))\n",
    "print(n_duplicates, \"duplicated lemmas with differing POS-tags have been removed.\")\n",
    "senti_lex_df.drop_duplicates(subset = ['lemma'], keep = 'first', inplace = True, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        POS                          lemma  polarity\n",
       "0       ADJ                              1 -1.000000\n",
       "1       ADJ                  a_bassa_quota -1.000000\n",
       "2       ADJ            a_bassa_risoluzione  1.000000\n",
       "3       ADJ  a_basso_contenuto_tecnologico  1.000000\n",
       "4       ADJ                a_basso_livello -1.000000\n",
       "...     ...                            ...       ...\n",
       "41806  VERB                     voler_bene  0.688083\n",
       "41807  VERB                        volerci  1.000000\n",
       "41808  VERB                      vulnerare -1.000000\n",
       "41809  VERB                        zoomare  1.000000\n",
       "41810  VERB                         zumare  1.000000\n",
       "\n",
       "[41811 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>POS</th>\n      <th>lemma</th>\n      <th>polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ADJ</td>\n      <td>1</td>\n      <td>-1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ADJ</td>\n      <td>a_bassa_quota</td>\n      <td>-1.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ADJ</td>\n      <td>a_bassa_risoluzione</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ADJ</td>\n      <td>a_basso_contenuto_tecnologico</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ADJ</td>\n      <td>a_basso_livello</td>\n      <td>-1.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41806</th>\n      <td>VERB</td>\n      <td>voler_bene</td>\n      <td>0.688083</td>\n    </tr>\n    <tr>\n      <th>41807</th>\n      <td>VERB</td>\n      <td>volerci</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>41808</th>\n      <td>VERB</td>\n      <td>vulnerare</td>\n      <td>-1.000000</td>\n    </tr>\n    <tr>\n      <th>41809</th>\n      <td>VERB</td>\n      <td>zoomare</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>41810</th>\n      <td>VERB</td>\n      <td>zumare</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>41811 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# Take a look at the dataframe containing the sentiment lexicon\n",
    "senti_lex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary out of the sentiment lexicon\n",
    "senti_lex_dict = {}\n",
    "for index, row in senti_lex_df.iterrows():\n",
    "    senti_lex_dict[row['lemma']] = {'POS': str(row['POS']), 'polarity': float(row['polarity'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "coronavirus: {'POS': 'NOUN', 'polarity': -1.0}\ncovid: {'POS': 'NOUN', 'polarity': -1.0}\nvirus: {'POS': 'NOUN', 'polarity': -1.0}\ninfezione: {'POS': 'NOUN', 'polarity': -0.5}\ncontagio: {'POS': 'NOUN', 'polarity': -0.5}\ncontaminazione: {'POS': 'NOUN', 'polarity': -0.5}\ninfettare: {'POS': 'VERB', 'polarity': -0.5}\ncontagiare: {'POS': 'VERB', 'polarity': -0.5}\ninfettato: {'POS': 'ADJ', 'polarity': -0.5}\ninfettivo: {'POS': 'ADJ', 'polarity': -0.5}\ncontagioso: {'POS': 'ADJ', 'polarity': -0.5}\nvirulento: {'POS': 'ADJ', 'polarity': -0.5}\npandemia: {'POS': 'NOUN', 'polarity': -0.675}\nepidemia: {'POS': 'NOUN', 'polarity': -0.675}\nlockdown: {'POS': 'NOUN', 'polarity': -1.0}\ncoprifuoco: {'POS': 'NOUN', 'polarity': -1.0}\ncrisi: {'POS': 'NOUN', 'polarity': -0.75}\nquarantena: {'POS': 'NOUN', 'polarity': -1.0}\ncontumacia: {'POS': 'NOUN', 'polarity': -1.0}\nricovero: {'POS': 'NOUN', 'polarity': -0.5}\nmalattia: {'POS': 'NOUN', 'polarity': -0.27}\ninfermità: {'POS': 'NOUN', 'polarity': -0.27}\nmorbo: {'POS': 'NOUN', 'polarity': -0.27}\n"
     ]
    }
   ],
   "source": [
    "# Check whether the newly added words are indeed contained in the lexicon\n",
    "added_words = ['coronavirus', 'covid', 'virus', 'infezione', 'contagio', 'contaminazione', 'infettare', 'contagiare', 'infettato', 'infettivo', 'contagioso', \n",
    "               'virulento', 'pandemia', 'epidemia', 'lockdown', 'coprifuoco', 'crisi', 'quarantena', 'contumacia', 'ricovero', 'malattia', 'infermità', 'morbo']\n",
    "for word in added_words:\n",
    "    print(word, \": \", senti_lex_dict[word], sep = '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary variables to save RAM\n",
    "del senti_lex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing time to evaluate the article sentiments: 0.07138670682907104 minutes\n"
     ]
    }
   ],
   "source": [
    "# Set up a NaiveSentimentClassifierIT object\n",
    "NSC_it = NaiveSentimentClassifierIT(senti_lex_dict, negations, booster_dic)\n",
    "# Evaluate the sentiment of the Italian articles\n",
    "Naive_tx_polarity = NSC_it.evaluate(it_tx, it_idx.it_idx.values.tolist(), name_output_file = 'it_naive_polarity_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Naive_polarity\n",
       "313578         0.223737\n",
       "460527         0.066695\n",
       "460528         0.027784\n",
       "460529         0.375276\n",
       "460530         0.077931\n",
       "...                 ...\n",
       "2425111        0.325523\n",
       "2425112        0.222156\n",
       "2425113        0.072668\n",
       "2425114       -0.018780\n",
       "2425115       -0.094737\n",
       "\n",
       "[23621 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Naive_polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>313578</th>\n      <td>0.223737</td>\n    </tr>\n    <tr>\n      <th>460527</th>\n      <td>0.066695</td>\n    </tr>\n    <tr>\n      <th>460528</th>\n      <td>0.027784</td>\n    </tr>\n    <tr>\n      <th>460529</th>\n      <td>0.375276</td>\n    </tr>\n    <tr>\n      <th>460530</th>\n      <td>0.077931</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2425111</th>\n      <td>0.325523</td>\n    </tr>\n    <tr>\n      <th>2425112</th>\n      <td>0.222156</td>\n    </tr>\n    <tr>\n      <th>2425113</th>\n      <td>0.072668</td>\n    </tr>\n    <tr>\n      <th>2425114</th>\n      <td>-0.018780</td>\n    </tr>\n    <tr>\n      <th>2425115</th>\n      <td>-0.094737</td>\n    </tr>\n  </tbody>\n</table>\n<p>23621 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# Take a look at the results\n",
    "Naive_tx_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The share of articles with a positive sentiment is 86.0 %\nThe share of articles with a negative sentiment is 14.0 %\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Naive_polarity\n",
       "count       23621.000\n",
       "mean            0.196\n",
       "std             0.201\n",
       "min            -0.867\n",
       "25%             0.076\n",
       "50%             0.197\n",
       "75%             0.315\n",
       "max             1.000"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Naive_polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>23621.000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.196</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.201</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-0.867</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.076</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.197</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.315</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# Take a look at some summary statistics\n",
    "share_pos = np.round(np.sum(Naive_tx_polarity['Naive_polarity'] > 0) / len(Naive_tx_polarity),2)\n",
    "share_neg = np.round(np.sum(Naive_tx_polarity['Naive_polarity'] < 0) / len(Naive_tx_polarity),2)\n",
    "print('The share of articles with a positive sentiment is', np.round(100*share_pos,2),'%')\n",
    "print('The share of articles with a negative sentiment is', np.round(100*share_neg,2),'%')\n",
    "np.round(Naive_tx_polarity.describe(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the results back in\n",
    "Naive_tx_polarity = pd.read_csv(\"Sentiment/Naive/it_naive_polarity_2.csv\", index_col = 0, dtype = {'Naive_polarity': float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Naive_polarity\n",
       "313578         0.223737\n",
       "460527         0.066695\n",
       "460528         0.027784\n",
       "460529         0.375276\n",
       "460530         0.077931\n",
       "...                 ...\n",
       "2425111        0.325523\n",
       "2425112        0.222156\n",
       "2425113        0.072668\n",
       "2425114       -0.018780\n",
       "2425115       -0.094737\n",
       "\n",
       "[23621 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Naive_polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>313578</th>\n      <td>0.223737</td>\n    </tr>\n    <tr>\n      <th>460527</th>\n      <td>0.066695</td>\n    </tr>\n    <tr>\n      <th>460528</th>\n      <td>0.027784</td>\n    </tr>\n    <tr>\n      <th>460529</th>\n      <td>0.375276</td>\n    </tr>\n    <tr>\n      <th>460530</th>\n      <td>0.077931</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2425111</th>\n      <td>0.325523</td>\n    </tr>\n    <tr>\n      <th>2425112</th>\n      <td>0.222156</td>\n    </tr>\n    <tr>\n      <th>2425113</th>\n      <td>0.072668</td>\n    </tr>\n    <tr>\n      <th>2425114</th>\n      <td>-0.018780</td>\n    </tr>\n    <tr>\n      <th>2425115</th>\n      <td>-0.094737</td>\n    </tr>\n  </tbody>\n</table>\n<p>23621 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# Take a look at the read in results\n",
    "Naive_tx_polarity"
   ]
  }
 ]
}